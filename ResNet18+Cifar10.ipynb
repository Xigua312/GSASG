{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for **ResNet18+Cifar10**\n",
    "### This notebook is for seed $42$. The results in the paper are the average of $5$ times with seeds in $[1, 19, 31, 42, 80]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"resnet in pytorch\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n",
    "    Deep Residual Learning for Image Recognition\n",
    "    https://arxiv.org/abs/1512.03385v1\n",
    "\"\"\"\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet18_copy():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(dataset_name, n_workers, batch_size):\n",
    "    train_data, test_data = load_data(dataset_name)\n",
    "    train_loader_workers = dict()\n",
    "    n = len(train_data)\n",
    "    \n",
    "    # preparing iterators for workers\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    a = np.int64(np.floor(n / n_workers))\n",
    "    top_ind = a * n_workers\n",
    "    seq = range(a, top_ind, a)\n",
    "    split = np.split(indices[:top_ind], seq)\n",
    "    b = 0\n",
    "    for ind in split:\n",
    "        train_loader_workers[b] = DataLoader(Subset(train_data, ind), batch_size=batch_size, shuffle=True)\n",
    "        b = b + 1\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_workers, test_loader\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'mnist':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        train_data = datasets.MNIST(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "        test_data = datasets.MNIST(root='./data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "\n",
    "    elif dataset_name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        train_data = datasets.CIFAR10(root='./data', train=True,\n",
    "                                      download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR10(root='./data', train=False,\n",
    "                                     download=True, transform=transform_test)\n",
    "\n",
    "    elif dataset_name == 'cifar100':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        train_data = datasets.CIFAR100(root='./data', train=True,\n",
    "                                       download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR100(root='./data', train=False,\n",
    "                                      download=True, transform=transform_test)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(dataset_name + ' is not known.')\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 30\n",
    "NUM_WORKERS = 10\n",
    "D = 10  # compute thread\n",
    "h = 0.01  # sparse level 99%\n",
    "h_g = 0.005  # sparse level 99%\n",
    "\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \n",
    "def evaluate_accuracy(model, data_iter, device):\n",
    "    correct = 0\n",
    "    for image, label in data_iter:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        output = model(image)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    accuracy = 100. * correct / len(data_iter.dataset)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Top_k sparse\n",
    "def prep_grad(x):\n",
    "    x_flat = torch.unsqueeze(x, 0).flatten()\n",
    "    dim = x.shape\n",
    "    d = x_flat.shape[0]\n",
    "    return x_flat, dim, d\n",
    "\n",
    "\n",
    "def top_k_opt(x, h):\n",
    "    \"\"\"\n",
    "    :param x: vector to sparsify\n",
    "    :param h: density\n",
    "    :return: compressed vector\n",
    "    \"\"\"\n",
    "    x, dim, d = prep_grad(x)\n",
    "    # number of coordinates kept\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    # positions of top_k coordinates\n",
    "    _, ind = torch.topk(torch.abs(x), r)\n",
    "    mask = torch.zeros_like(x)\n",
    "    mask[ind] = 1\n",
    "    t = mask * x\n",
    "    t = t.reshape(dim)\n",
    "    return t\n",
    "\n",
    "\n",
    "def compt(old, new):\n",
    "    result = 0.0\n",
    "    for i in range(len(old)):\n",
    "        result += ((old[i].view(-1) - new[i].view(-1)) ** 2).sum().item()\n",
    "    return result\n",
    "\n",
    "\n",
    "def adjust_lr(epoch_input):\n",
    "    lr_ad, lr_th = 0.01, 0.01\n",
    "    if epoch_input >= 20:\n",
    "        lr_ad = 0.001\n",
    "    if epoch_input > 20:\n",
    "        lr_th = 0.001\n",
    "    return lr_ad, lr_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASGS  lr:0.01--h:0.01--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "Element_parameter_sparse: 111723\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.2899\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 1.6481\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.2915\n"
     ]
    }
   ],
   "source": [
    "print(\"TASGS  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "model_copy = resnet18_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行  新加的！！！！\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]# 新加的！！！！\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    \n",
    "    # adjust learning_rate\n",
    "    lr, lr_thead = adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "    \n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)\n",
    "                        grad_agg[p_id] += Tk_sparse\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        GTk_sparse = top_k_opt(grad_worker[w_id][p_id], h)#111111\n",
    "                        grad_gtagg[p_id] += GTk_sparse#11111111\n",
    "                        error_gtworker[w_id][p_id] = error_worker[w_id][p_id] + (Tk_sparse - GTk_sparse)#1111111\n",
    "                        grad_gtworker[w_id][p_id] = GTk_sparse#1111111\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (lr_thead * (NUM_WORKERS ** 2))\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                            GTk_sparse = top_k_opt(grad_worker[w_id][p_id], h)#1111112\n",
    "                            grad_gtagg[p_id] += GTk_sparse#111111112\n",
    "                            error_gtworker[w_id][p_id] = error_worker[w_id][p_id] + (Tk_sparse - GTk_sparse)#11111112\n",
    "                            grad_gtworker[w_id][p_id] = GTk_sparse#11111112\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            grad_gtagg[p_id] += grad_gtworker[w_id][p_id]#11111111\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)#11111111\n",
    "                p.data.add_(grad_gtagg[p_id], alpha=-1)#11111\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 10 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "          \n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASG  lr:0.01--h:0.01--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "Element_parameter_sparse: 111723\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.5554\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 1.7179\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.4719\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 1.0864\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 0.7045\n",
      "epoch 1, skip_num 0, loss 1.4435, train acc 47.980%, test acc 63.510%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 1.2649\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 1.7896\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.0931\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 0.5187\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.3112\n",
      "epoch 2, skip_num 11, loss 0.9130, train acc 68.112%, test acc 74.490%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.7322\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 1.2160\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 1.0160\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.5394\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 0.9745\n",
      "epoch 3, skip_num 72, loss 0.7164, train acc 75.354%, test acc 79.910%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.4484\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.5753\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 1.5537\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 0.4945\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 1.0145\n",
      "epoch 4, skip_num 186, loss 0.6116, train acc 78.838%, test acc 82.010%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.3338\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.6019\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.1295\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.3880\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 0.8534\n",
      "epoch 5, skip_num 278, loss 0.5350, train acc 81.660%, test acc 83.050%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.2877\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.3625\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 1.3265\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.6245\n"
     ]
    }
   ],
   "source": [
    "print(\"SASG  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "model_copy = resnet18_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    \n",
    "    # adjust learning_rate\n",
    "    lr, lr_thead = adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "    \n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)\n",
    "                        grad_agg[p_id] += Tk_sparse\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (lr_thead * (NUM_WORKERS ** 2))\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 10 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "          \n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SASG-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SASG-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASG  lr:0.01--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.8352\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 1.3243\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.3388\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 1.9584\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 1.5281\n",
      "epoch 1, skip_num 0, loss 1.7299, train acc 37.644%, test acc 46.010%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 1.5539\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 1.2017\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.3192\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 0.7969\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.4362\n",
      "epoch 2, skip_num 0, loss 1.1630, train acc 58.488%, test acc 68.800%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.7194\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 1.2678\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 0.8897\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.4584\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 0.9292\n",
      "epoch 3, skip_num 0, loss 0.9014, train acc 68.666%, test acc 74.310%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.4489\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.7567\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 1.1988\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 0.8462\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 0.9641\n",
      "epoch 4, skip_num 2, loss 0.7498, train acc 74.068%, test acc 79.210%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.5821\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.6292\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.2570\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.5041\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 0.9597\n",
      "epoch 5, skip_num 3, loss 0.6536, train acc 77.616%, test acc 78.990%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.9511\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.2691\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 0.8870\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.8665\n",
      "Epoch: 006/030 | Batch 0500/0500 | Cost: 0.3911\n",
      "epoch 6, skip_num 12, loss 0.5809, train acc 80.126%, test acc 82.810%\n",
      "EPOCH:  7 learning rate:  0.01\n",
      "Epoch: 007/030 | Batch 0100/0500 | Cost: 0.0925\n",
      "Epoch: 007/030 | Batch 0200/0500 | Cost: 0.6198\n",
      "Epoch: 007/030 | Batch 0300/0500 | Cost: 0.2359\n",
      "Epoch: 007/030 | Batch 0400/0500 | Cost: 0.3055\n",
      "Epoch: 007/030 | Batch 0500/0500 | Cost: 0.4975\n",
      "epoch 7, skip_num 11, loss 0.5202, train acc 82.098%, test acc 83.410%\n",
      "EPOCH:  8 learning rate:  0.01\n",
      "Epoch: 008/030 | Batch 0100/0500 | Cost: 0.7965\n",
      "Epoch: 008/030 | Batch 0200/0500 | Cost: 0.6555\n",
      "Epoch: 008/030 | Batch 0300/0500 | Cost: 0.9726\n",
      "Epoch: 008/030 | Batch 0400/0500 | Cost: 0.4376\n",
      "Epoch: 008/030 | Batch 0500/0500 | Cost: 0.5735\n",
      "epoch 8, skip_num 35, loss 0.4786, train acc 83.430%, test acc 83.010%\n",
      "EPOCH:  9 learning rate:  0.01\n",
      "Epoch: 009/030 | Batch 0100/0500 | Cost: 0.4508\n",
      "Epoch: 009/030 | Batch 0200/0500 | Cost: 0.4060\n",
      "Epoch: 009/030 | Batch 0300/0500 | Cost: 0.2790\n",
      "Epoch: 009/030 | Batch 0400/0500 | Cost: 0.4031\n",
      "Epoch: 009/030 | Batch 0500/0500 | Cost: 0.3532\n",
      "epoch 9, skip_num 50, loss 0.4361, train acc 84.976%, test acc 85.260%\n",
      "EPOCH:  10 learning rate:  0.01\n",
      "Epoch: 010/030 | Batch 0100/0500 | Cost: 0.1016\n",
      "Epoch: 010/030 | Batch 0200/0500 | Cost: 0.5107\n",
      "Epoch: 010/030 | Batch 0300/0500 | Cost: 0.1476\n",
      "Epoch: 010/030 | Batch 0400/0500 | Cost: 0.1215\n",
      "Epoch: 010/030 | Batch 0500/0500 | Cost: 0.1235\n",
      "epoch 10, skip_num 75, loss 0.4087, train acc 85.796%, test acc 85.340%\n",
      "EPOCH:  11 learning rate:  0.01\n",
      "Epoch: 011/030 | Batch 0100/0500 | Cost: 0.3698\n",
      "Epoch: 011/030 | Batch 0200/0500 | Cost: 0.4147\n",
      "Epoch: 011/030 | Batch 0300/0500 | Cost: 0.0613\n",
      "Epoch: 011/030 | Batch 0400/0500 | Cost: 0.5906\n",
      "Epoch: 011/030 | Batch 0500/0500 | Cost: 0.5701\n",
      "epoch 11, skip_num 91, loss 0.3809, train acc 86.884%, test acc 86.330%\n",
      "EPOCH:  12 learning rate:  0.01\n",
      "Epoch: 012/030 | Batch 0100/0500 | Cost: 0.4871\n",
      "Epoch: 012/030 | Batch 0200/0500 | Cost: 0.3112\n",
      "Epoch: 012/030 | Batch 0300/0500 | Cost: 0.9991\n",
      "Epoch: 012/030 | Batch 0400/0500 | Cost: 0.7421\n",
      "Epoch: 012/030 | Batch 0500/0500 | Cost: 0.3618\n",
      "epoch 12, skip_num 113, loss 0.3554, train acc 87.726%, test acc 88.080%\n",
      "EPOCH:  13 learning rate:  0.01\n",
      "Epoch: 013/030 | Batch 0100/0500 | Cost: 0.2191\n",
      "Epoch: 013/030 | Batch 0200/0500 | Cost: 0.1168\n",
      "Epoch: 013/030 | Batch 0300/0500 | Cost: 0.9275\n",
      "Epoch: 013/030 | Batch 0400/0500 | Cost: 0.1468\n",
      "Epoch: 013/030 | Batch 0500/0500 | Cost: 0.0356\n",
      "epoch 13, skip_num 161, loss 0.3337, train acc 88.504%, test acc 87.150%\n",
      "EPOCH:  14 learning rate:  0.01\n",
      "Epoch: 014/030 | Batch 0100/0500 | Cost: 0.3060\n",
      "Epoch: 014/030 | Batch 0200/0500 | Cost: 0.5045\n",
      "Epoch: 014/030 | Batch 0300/0500 | Cost: 0.1524\n",
      "Epoch: 014/030 | Batch 0400/0500 | Cost: 0.2259\n",
      "Epoch: 014/030 | Batch 0500/0500 | Cost: 0.0374\n",
      "epoch 14, skip_num 178, loss 0.3130, train acc 89.044%, test acc 87.740%\n",
      "EPOCH:  15 learning rate:  0.01\n",
      "Epoch: 015/030 | Batch 0100/0500 | Cost: 0.0553\n",
      "Epoch: 015/030 | Batch 0200/0500 | Cost: 0.1047\n",
      "Epoch: 015/030 | Batch 0300/0500 | Cost: 0.0981\n",
      "Epoch: 015/030 | Batch 0400/0500 | Cost: 0.0363\n",
      "Epoch: 015/030 | Batch 0500/0500 | Cost: 0.8981\n",
      "epoch 15, skip_num 216, loss 0.2885, train acc 90.102%, test acc 87.840%\n",
      "EPOCH:  16 learning rate:  0.01\n",
      "Epoch: 016/030 | Batch 0100/0500 | Cost: 0.1051\n",
      "Epoch: 016/030 | Batch 0200/0500 | Cost: 0.3514\n",
      "Epoch: 016/030 | Batch 0300/0500 | Cost: 0.2755\n",
      "Epoch: 016/030 | Batch 0400/0500 | Cost: 0.4496\n",
      "Epoch: 016/030 | Batch 0500/0500 | Cost: 0.2121\n",
      "epoch 16, skip_num 210, loss 0.2753, train acc 90.534%, test acc 88.460%\n",
      "EPOCH:  17 learning rate:  0.01\n",
      "Epoch: 017/030 | Batch 0100/0500 | Cost: 0.4977\n",
      "Epoch: 017/030 | Batch 0200/0500 | Cost: 0.0437\n",
      "Epoch: 017/030 | Batch 0300/0500 | Cost: 0.3692\n",
      "Epoch: 017/030 | Batch 0400/0500 | Cost: 0.0846\n",
      "Epoch: 017/030 | Batch 0500/0500 | Cost: 0.1510\n",
      "epoch 17, skip_num 265, loss 0.2632, train acc 90.812%, test acc 88.400%\n",
      "EPOCH:  18 learning rate:  0.01\n",
      "Epoch: 018/030 | Batch 0100/0500 | Cost: 0.1217\n",
      "Epoch: 018/030 | Batch 0200/0500 | Cost: 0.3824\n",
      "Epoch: 018/030 | Batch 0300/0500 | Cost: 0.5520\n",
      "Epoch: 018/030 | Batch 0400/0500 | Cost: 0.2688\n",
      "Epoch: 018/030 | Batch 0500/0500 | Cost: 0.2312\n",
      "epoch 18, skip_num 331, loss 0.2554, train acc 91.212%, test acc 89.620%\n",
      "EPOCH:  19 learning rate:  0.01\n",
      "Epoch: 019/030 | Batch 0100/0500 | Cost: 0.1389\n",
      "Epoch: 019/030 | Batch 0200/0500 | Cost: 0.3966\n",
      "Epoch: 019/030 | Batch 0300/0500 | Cost: 0.1291\n",
      "Epoch: 019/030 | Batch 0400/0500 | Cost: 0.1916\n",
      "Epoch: 019/030 | Batch 0500/0500 | Cost: 0.0363\n",
      "epoch 19, skip_num 382, loss 0.2383, train acc 91.668%, test acc 88.930%\n",
      "EPOCH:  20 learning rate:  0.01\n",
      "Epoch: 020/030 | Batch 0100/0500 | Cost: 0.1247\n",
      "Epoch: 020/030 | Batch 0200/0500 | Cost: 0.2563\n",
      "Epoch: 020/030 | Batch 0300/0500 | Cost: 0.8639\n",
      "Epoch: 020/030 | Batch 0400/0500 | Cost: 0.1823\n",
      "Epoch: 020/030 | Batch 0500/0500 | Cost: 0.1547\n",
      "epoch 20, skip_num 380, loss 0.2290, train acc 92.116%, test acc 87.250%\n",
      "EPOCH:  21 learning rate:  0.001\n",
      "Epoch: 021/030 | Batch 0100/0500 | Cost: 0.1832\n",
      "Epoch: 021/030 | Batch 0200/0500 | Cost: 0.1856\n",
      "Epoch: 021/030 | Batch 0300/0500 | Cost: 0.1250\n",
      "Epoch: 021/030 | Batch 0400/0500 | Cost: 0.6027\n",
      "Epoch: 021/030 | Batch 0500/0500 | Cost: 0.1818\n",
      "epoch 21, skip_num 397, loss 0.1502, train acc 95.014%, test acc 91.820%\n",
      "EPOCH:  22 learning rate:  0.001\n",
      "Epoch: 022/030 | Batch 0100/0500 | Cost: 0.1009\n",
      "Epoch: 022/030 | Batch 0200/0500 | Cost: 0.0840\n",
      "Epoch: 022/030 | Batch 0300/0500 | Cost: 0.2174\n",
      "Epoch: 022/030 | Batch 0400/0500 | Cost: 0.0213\n",
      "Epoch: 022/030 | Batch 0500/0500 | Cost: 0.0334\n",
      "****************************************************************************************************\n",
      "Iter_num: 11000 Test_acc 92.09 Skip_round: 4448 Comm_round: 105552\n",
      "****************************************************************************************************\n",
      "epoch 22, skip_num 1536, loss 0.1323, train acc 95.572%, test acc 92.090%\n",
      "EPOCH:  23 learning rate:  0.001\n",
      "Epoch: 023/030 | Batch 0100/0500 | Cost: 0.1689\n",
      "Epoch: 023/030 | Batch 0200/0500 | Cost: 0.0211\n",
      "Epoch: 023/030 | Batch 0300/0500 | Cost: 0.1027\n",
      "Epoch: 023/030 | Batch 0400/0500 | Cost: 0.1112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023/030 | Batch 0500/0500 | Cost: 0.0216\n",
      "epoch 23, skip_num 1504, loss 0.1235, train acc 95.766%, test acc 91.950%\n",
      "EPOCH:  24 learning rate:  0.001\n",
      "Epoch: 024/030 | Batch 0100/0500 | Cost: 0.2997\n",
      "Epoch: 024/030 | Batch 0200/0500 | Cost: 0.5897\n",
      "Epoch: 024/030 | Batch 0300/0500 | Cost: 0.0313\n",
      "Epoch: 024/030 | Batch 0400/0500 | Cost: 0.0227\n",
      "Epoch: 024/030 | Batch 0500/0500 | Cost: 0.1856\n",
      "epoch 24, skip_num 1578, loss 0.1172, train acc 96.010%, test acc 91.930%\n",
      "EPOCH:  25 learning rate:  0.001\n",
      "Epoch: 025/030 | Batch 0100/0500 | Cost: 0.5145\n",
      "Epoch: 025/030 | Batch 0200/0500 | Cost: 0.2117\n",
      "Epoch: 025/030 | Batch 0300/0500 | Cost: 0.0517\n",
      "Epoch: 025/030 | Batch 0400/0500 | Cost: 0.1884\n",
      "Epoch: 025/030 | Batch 0500/0500 | Cost: 0.0085\n",
      "epoch 25, skip_num 1594, loss 0.1115, train acc 96.326%, test acc 92.250%\n",
      "EPOCH:  26 learning rate:  0.001\n",
      "Epoch: 026/030 | Batch 0100/0500 | Cost: 0.0154\n",
      "Epoch: 026/030 | Batch 0200/0500 | Cost: 0.1320\n",
      "Epoch: 026/030 | Batch 0300/0500 | Cost: 0.0607\n",
      "Epoch: 026/030 | Batch 0400/0500 | Cost: 0.4201\n",
      "Epoch: 026/030 | Batch 0500/0500 | Cost: 0.0164\n",
      "epoch 26, skip_num 1576, loss 0.1091, train acc 96.368%, test acc 92.130%\n",
      "EPOCH:  27 learning rate:  0.001\n",
      "Epoch: 027/030 | Batch 0100/0500 | Cost: 0.0123\n",
      "Epoch: 027/030 | Batch 0200/0500 | Cost: 0.0389\n",
      "Epoch: 027/030 | Batch 0300/0500 | Cost: 0.1922\n",
      "Epoch: 027/030 | Batch 0400/0500 | Cost: 0.1084\n",
      "Epoch: 027/030 | Batch 0500/0500 | Cost: 0.0328\n",
      "epoch 27, skip_num 1654, loss 0.1021, train acc 96.622%, test acc 92.200%\n",
      "EPOCH:  28 learning rate:  0.001\n",
      "Epoch: 028/030 | Batch 0100/0500 | Cost: 0.3151\n",
      "Epoch: 028/030 | Batch 0200/0500 | Cost: 0.0319\n",
      "Epoch: 028/030 | Batch 0300/0500 | Cost: 0.0171\n",
      "Epoch: 028/030 | Batch 0400/0500 | Cost: 0.1323\n",
      "Epoch: 028/030 | Batch 0500/0500 | Cost: 0.4035\n",
      "epoch 28, skip_num 1662, loss 0.1016, train acc 96.636%, test acc 91.920%\n",
      "EPOCH:  29 learning rate:  0.001\n",
      "Epoch: 029/030 | Batch 0100/0500 | Cost: 0.1600\n",
      "Epoch: 029/030 | Batch 0200/0500 | Cost: 0.0335\n",
      "Epoch: 029/030 | Batch 0300/0500 | Cost: 0.0163\n",
      "Epoch: 029/030 | Batch 0400/0500 | Cost: 0.0336\n",
      "Epoch: 029/030 | Batch 0500/0500 | Cost: 0.0483\n",
      "epoch 29, skip_num 1587, loss 0.0961, train acc 96.836%, test acc 92.280%\n",
      "EPOCH:  30 learning rate:  0.001\n",
      "Epoch: 030/030 | Batch 0100/0500 | Cost: 0.0531\n",
      "Epoch: 030/030 | Batch 0200/0500 | Cost: 0.0563\n",
      "Epoch: 030/030 | Batch 0300/0500 | Cost: 0.0220\n",
      "Epoch: 030/030 | Batch 0400/0500 | Cost: 0.0072\n",
      "Epoch: 030/030 | Batch 0500/0500 | Cost: 0.0261\n",
      "epoch 30, skip_num 1697, loss 0.0932, train acc 96.852%, test acc 92.180%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"LASG  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "model_copy = resnet18_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    \n",
    "    # adjust learning_rate\n",
    "    lr, lr_thead = adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach()\n",
    "                        grad_agg[p_id] += g\n",
    "                        grad_worker[w_id][p_id] = g\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (lr_thead * (NUM_WORKERS ** 2))\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach()\n",
    "                            grad_agg[p_id] += g\n",
    "                            grad_worker[w_id][p_id] = g\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 10 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"LASG-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"LASG-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse  lr:0.001--h:0.01--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.4262\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 0.9658\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.9845\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 0.9125\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 0.8422\n",
      "epoch 1, loss 1.4691, train acc 46.694%, test acc 64.420%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 0.7621\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 0.7830\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.4169\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 1.0121\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.3188\n",
      "epoch 2, loss 0.9391, train acc 67.336%, test acc 75.170%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.9401\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 0.6797\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 0.1498\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.2955\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 0.3393\n",
      "epoch 3, loss 0.7241, train acc 75.186%, test acc 79.300%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.4862\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.5543\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 0.3812\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 0.8400\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 0.3888\n",
      "epoch 4, loss 0.6031, train acc 79.070%, test acc 82.880%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.7157\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.2523\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.1260\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.4649\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 0.0596\n",
      "epoch 5, loss 0.5349, train acc 81.546%, test acc 83.620%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.2219\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.2172\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 0.0606\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.2149\n",
      "Epoch: 006/030 | Batch 0500/0500 | Cost: 0.3497\n",
      "epoch 6, loss 0.4720, train acc 83.774%, test acc 84.830%\n",
      "EPOCH:  7 learning rate:  0.01\n",
      "Epoch: 007/030 | Batch 0100/0500 | Cost: 0.6502\n",
      "Epoch: 007/030 | Batch 0200/0500 | Cost: 0.4722\n",
      "Epoch: 007/030 | Batch 0300/0500 | Cost: 0.0709\n",
      "Epoch: 007/030 | Batch 0400/0500 | Cost: 0.2298\n",
      "Epoch: 007/030 | Batch 0500/0500 | Cost: 0.4153\n",
      "epoch 7, loss 0.4242, train acc 85.380%, test acc 85.300%\n",
      "EPOCH:  8 learning rate:  0.01\n",
      "Epoch: 008/030 | Batch 0100/0500 | Cost: 0.4294\n",
      "Epoch: 008/030 | Batch 0200/0500 | Cost: 0.4128\n",
      "Epoch: 008/030 | Batch 0300/0500 | Cost: 0.3139\n",
      "Epoch: 008/030 | Batch 0400/0500 | Cost: 0.4444\n",
      "Epoch: 008/030 | Batch 0500/0500 | Cost: 0.7061\n",
      "epoch 8, loss 0.3881, train acc 86.636%, test acc 85.920%\n",
      "EPOCH:  9 learning rate:  0.01\n",
      "Epoch: 009/030 | Batch 0100/0500 | Cost: 0.6176\n",
      "Epoch: 009/030 | Batch 0200/0500 | Cost: 0.0950\n",
      "Epoch: 009/030 | Batch 0300/0500 | Cost: 0.4806\n",
      "Epoch: 009/030 | Batch 0400/0500 | Cost: 0.3890\n",
      "Epoch: 009/030 | Batch 0500/0500 | Cost: 0.0608\n",
      "epoch 9, loss 0.3587, train acc 87.660%, test acc 87.490%\n",
      "EPOCH:  10 learning rate:  0.01\n",
      "Epoch: 010/030 | Batch 0100/0500 | Cost: 0.1851\n",
      "Epoch: 010/030 | Batch 0200/0500 | Cost: 0.6198\n",
      "Epoch: 010/030 | Batch 0300/0500 | Cost: 0.1886\n",
      "Epoch: 010/030 | Batch 0400/0500 | Cost: 0.4623\n",
      "Epoch: 010/030 | Batch 0500/0500 | Cost: 0.4572\n",
      "epoch 10, loss 0.3272, train acc 88.780%, test acc 87.170%\n",
      "EPOCH:  11 learning rate:  0.01\n",
      "Epoch: 011/030 | Batch 0100/0500 | Cost: 0.2679\n",
      "Epoch: 011/030 | Batch 0200/0500 | Cost: 0.3429\n",
      "Epoch: 011/030 | Batch 0300/0500 | Cost: 0.4187\n",
      "Epoch: 011/030 | Batch 0400/0500 | Cost: 0.5399\n",
      "Epoch: 011/030 | Batch 0500/0500 | Cost: 0.5128\n",
      "epoch 11, loss 0.3056, train acc 89.456%, test acc 88.050%\n",
      "EPOCH:  12 learning rate:  0.01\n",
      "Epoch: 012/030 | Batch 0100/0500 | Cost: 0.5770\n",
      "Epoch: 012/030 | Batch 0200/0500 | Cost: 0.2027\n",
      "Epoch: 012/030 | Batch 0300/0500 | Cost: 0.4933\n",
      "Epoch: 012/030 | Batch 0400/0500 | Cost: 0.4918\n",
      "Epoch: 012/030 | Batch 0500/0500 | Cost: 0.4772\n",
      "epoch 12, loss 0.2809, train acc 90.224%, test acc 88.790%\n",
      "EPOCH:  13 learning rate:  0.01\n",
      "Epoch: 013/030 | Batch 0100/0500 | Cost: 0.4625\n",
      "Epoch: 013/030 | Batch 0200/0500 | Cost: 0.5898\n",
      "Epoch: 013/030 | Batch 0300/0500 | Cost: 0.2276\n",
      "Epoch: 013/030 | Batch 0400/0500 | Cost: 0.4009\n",
      "Epoch: 013/030 | Batch 0500/0500 | Cost: 0.3587\n",
      "epoch 13, loss 0.2658, train acc 90.880%, test acc 88.530%\n",
      "EPOCH:  14 learning rate:  0.01\n",
      "Epoch: 014/030 | Batch 0100/0500 | Cost: 0.0755\n",
      "Epoch: 014/030 | Batch 0200/0500 | Cost: 0.1125\n",
      "Epoch: 014/030 | Batch 0300/0500 | Cost: 0.0474\n",
      "Epoch: 014/030 | Batch 0400/0500 | Cost: 0.1006\n",
      "Epoch: 014/030 | Batch 0500/0500 | Cost: 0.0213\n",
      "epoch 14, loss 0.2507, train acc 91.290%, test acc 89.600%\n",
      "EPOCH:  15 learning rate:  0.01\n",
      "Epoch: 015/030 | Batch 0100/0500 | Cost: 0.0362\n",
      "Epoch: 015/030 | Batch 0200/0500 | Cost: 0.3203\n",
      "Epoch: 015/030 | Batch 0300/0500 | Cost: 0.0880\n",
      "Epoch: 015/030 | Batch 0400/0500 | Cost: 0.4163\n",
      "Epoch: 015/030 | Batch 0500/0500 | Cost: 0.0779\n",
      "epoch 15, loss 0.2323, train acc 91.932%, test acc 89.590%\n",
      "EPOCH:  16 learning rate:  0.01\n",
      "Epoch: 016/030 | Batch 0100/0500 | Cost: 0.3234\n",
      "Epoch: 016/030 | Batch 0200/0500 | Cost: 0.1541\n",
      "Epoch: 016/030 | Batch 0300/0500 | Cost: 0.0283\n",
      "Epoch: 016/030 | Batch 0400/0500 | Cost: 0.1910\n",
      "Epoch: 016/030 | Batch 0500/0500 | Cost: 0.0137\n",
      "epoch 16, loss 0.2196, train acc 92.382%, test acc 89.470%\n",
      "EPOCH:  17 learning rate:  0.01\n",
      "Epoch: 017/030 | Batch 0100/0500 | Cost: 0.0946\n",
      "Epoch: 017/030 | Batch 0200/0500 | Cost: 0.1170\n",
      "Epoch: 017/030 | Batch 0300/0500 | Cost: 0.6891\n",
      "Epoch: 017/030 | Batch 0400/0500 | Cost: 0.2379\n",
      "Epoch: 017/030 | Batch 0500/0500 | Cost: 0.4213\n",
      "epoch 17, loss 0.2045, train acc 92.952%, test acc 89.720%\n",
      "EPOCH:  18 learning rate:  0.01\n",
      "Epoch: 018/030 | Batch 0100/0500 | Cost: 0.4324\n",
      "Epoch: 018/030 | Batch 0200/0500 | Cost: 0.0681\n",
      "Epoch: 018/030 | Batch 0300/0500 | Cost: 0.1651\n",
      "Epoch: 018/030 | Batch 0400/0500 | Cost: 0.5853\n",
      "Epoch: 018/030 | Batch 0500/0500 | Cost: 0.3175\n",
      "epoch 18, loss 0.1916, train acc 93.470%, test acc 89.690%\n",
      "EPOCH:  19 learning rate:  0.01\n",
      "Epoch: 019/030 | Batch 0100/0500 | Cost: 0.0875\n",
      "Epoch: 019/030 | Batch 0200/0500 | Cost: 0.3533\n",
      "Epoch: 019/030 | Batch 0300/0500 | Cost: 0.1596\n",
      "Epoch: 019/030 | Batch 0400/0500 | Cost: 0.3088\n",
      "Epoch: 019/030 | Batch 0500/0500 | Cost: 0.4249\n",
      "epoch 19, loss 0.1831, train acc 93.600%, test acc 89.620%\n",
      "EPOCH:  20 learning rate:  0.01\n",
      "Epoch: 020/030 | Batch 0100/0500 | Cost: 0.0840\n",
      "Epoch: 020/030 | Batch 0200/0500 | Cost: 0.0103\n",
      "Epoch: 020/030 | Batch 0300/0500 | Cost: 0.0673\n",
      "Epoch: 020/030 | Batch 0400/0500 | Cost: 0.0916\n",
      "Epoch: 020/030 | Batch 0500/0500 | Cost: 0.0425\n",
      "epoch 20, loss 0.1709, train acc 94.104%, test acc 89.910%\n",
      "EPOCH:  21 learning rate:  0.001\n",
      "Epoch: 021/030 | Batch 0100/0500 | Cost: 0.5709\n",
      "Epoch: 021/030 | Batch 0200/0500 | Cost: 0.1296\n",
      "Epoch: 021/030 | Batch 0300/0500 | Cost: 0.0371\n",
      "Epoch: 021/030 | Batch 0400/0500 | Cost: 0.0513\n",
      "Epoch: 021/030 | Batch 0500/0500 | Cost: 0.0063\n",
      "epoch 21, loss 0.1242, train acc 95.816%, test acc 91.840%\n",
      "EPOCH:  22 learning rate:  0.001\n",
      "Epoch: 022/030 | Batch 0100/0500 | Cost: 0.0563\n",
      "****************************************************************************************************\n",
      "Iter_num: 10600 Test_acc 92.03 Comm_round: 106000\n",
      "****************************************************************************************************\n",
      "Epoch: 022/030 | Batch 0200/0500 | Cost: 0.0319\n",
      "Epoch: 022/030 | Batch 0300/0500 | Cost: 0.0364\n",
      "Epoch: 022/030 | Batch 0400/0500 | Cost: 0.0247\n",
      "Epoch: 022/030 | Batch 0500/0500 | Cost: 0.1095\n",
      "epoch 22, loss 0.0993, train acc 96.676%, test acc 91.930%\n",
      "EPOCH:  23 learning rate:  0.001\n",
      "Epoch: 023/030 | Batch 0100/0500 | Cost: 0.0016\n",
      "Epoch: 023/030 | Batch 0200/0500 | Cost: 0.1810\n",
      "Epoch: 023/030 | Batch 0300/0500 | Cost: 0.0825\n",
      "Epoch: 023/030 | Batch 0400/0500 | Cost: 0.0093\n",
      "Epoch: 023/030 | Batch 0500/0500 | Cost: 0.2153\n",
      "epoch 23, loss 0.0878, train acc 97.164%, test acc 91.970%\n",
      "EPOCH:  24 learning rate:  0.001\n",
      "Epoch: 024/030 | Batch 0100/0500 | Cost: 0.0043\n",
      "Epoch: 024/030 | Batch 0200/0500 | Cost: 0.0047\n",
      "Epoch: 024/030 | Batch 0300/0500 | Cost: 0.0567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024/030 | Batch 0400/0500 | Cost: 0.0044\n",
      "Epoch: 024/030 | Batch 0500/0500 | Cost: 0.0782\n",
      "epoch 24, loss 0.0819, train acc 97.286%, test acc 92.110%\n",
      "EPOCH:  25 learning rate:  0.001\n",
      "Epoch: 025/030 | Batch 0100/0500 | Cost: 0.0358\n",
      "Epoch: 025/030 | Batch 0200/0500 | Cost: 0.0261\n",
      "Epoch: 025/030 | Batch 0300/0500 | Cost: 0.1209\n",
      "Epoch: 025/030 | Batch 0400/0500 | Cost: 0.4155\n",
      "Epoch: 025/030 | Batch 0500/0500 | Cost: 0.0686\n",
      "epoch 25, loss 0.0787, train acc 97.430%, test acc 92.250%\n",
      "EPOCH:  26 learning rate:  0.001\n",
      "Epoch: 026/030 | Batch 0100/0500 | Cost: 0.0095\n",
      "Epoch: 026/030 | Batch 0200/0500 | Cost: 0.0120\n",
      "Epoch: 026/030 | Batch 0300/0500 | Cost: 0.0471\n",
      "Epoch: 026/030 | Batch 0400/0500 | Cost: 0.2734\n",
      "Epoch: 026/030 | Batch 0500/0500 | Cost: 0.0592\n",
      "epoch 26, loss 0.0759, train acc 97.494%, test acc 92.340%\n",
      "EPOCH:  27 learning rate:  0.001\n",
      "Epoch: 027/030 | Batch 0100/0500 | Cost: 0.0316\n",
      "Epoch: 027/030 | Batch 0200/0500 | Cost: 0.1314\n",
      "Epoch: 027/030 | Batch 0300/0500 | Cost: 0.2326\n",
      "Epoch: 027/030 | Batch 0400/0500 | Cost: 0.0174\n",
      "Epoch: 027/030 | Batch 0500/0500 | Cost: 0.0035\n",
      "epoch 27, loss 0.0730, train acc 97.516%, test acc 92.160%\n",
      "EPOCH:  28 learning rate:  0.001\n",
      "Epoch: 028/030 | Batch 0100/0500 | Cost: 0.0115\n",
      "Epoch: 028/030 | Batch 0200/0500 | Cost: 0.0670\n",
      "Epoch: 028/030 | Batch 0300/0500 | Cost: 0.0031\n",
      "Epoch: 028/030 | Batch 0400/0500 | Cost: 0.1699\n",
      "Epoch: 028/030 | Batch 0500/0500 | Cost: 0.0283\n",
      "epoch 28, loss 0.0694, train acc 97.712%, test acc 92.190%\n",
      "EPOCH:  29 learning rate:  0.001\n",
      "Epoch: 029/030 | Batch 0100/0500 | Cost: 0.2482\n",
      "Epoch: 029/030 | Batch 0200/0500 | Cost: 0.0136\n",
      "Epoch: 029/030 | Batch 0300/0500 | Cost: 0.0042\n",
      "Epoch: 029/030 | Batch 0400/0500 | Cost: 0.0318\n",
      "Epoch: 029/030 | Batch 0500/0500 | Cost: 0.1395\n",
      "epoch 29, loss 0.0664, train acc 97.816%, test acc 92.400%\n",
      "EPOCH:  30 learning rate:  0.001\n",
      "Epoch: 030/030 | Batch 0100/0500 | Cost: 0.0355\n",
      "Epoch: 030/030 | Batch 0200/0500 | Cost: 0.0037\n",
      "Epoch: 030/030 | Batch 0300/0500 | Cost: 0.0285\n",
      "Epoch: 030/030 | Batch 0400/0500 | Cost: 0.1252\n",
      "Epoch: 030/030 | Batch 0500/0500 | Cost: 0.0013\n",
      "epoch 30, loss 0.0661, train acc 97.802%, test acc 92.350%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparse  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "\n",
    "    # adjust learning_rate\n",
    "    lr, _= adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                    Tk_sparse = top_k_opt(g, h)\n",
    "                    grad_agg[p_id] += Tk_sparse\n",
    "                    error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 10)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dis-SGD  lr:0.001--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.4979\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 1.6422\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.8526\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 1.0593\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 1.1429\n",
      "epoch 1, loss 1.7717, train acc 37.058%, test acc 56.440%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 0.7146\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 1.2058\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.6205\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 1.4135\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.6881\n",
      "epoch 2, loss 1.1591, train acc 58.984%, test acc 69.150%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.8793\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 1.0879\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 0.2845\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.2265\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 0.5915\n",
      "epoch 3, loss 0.8877, train acc 69.014%, test acc 74.030%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.7978\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.5801\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 0.7101\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 0.7375\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 0.4764\n",
      "epoch 4, loss 0.7345, train acc 74.644%, test acc 78.300%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.8329\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.4700\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.2817\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.4549\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 0.2468\n",
      "epoch 5, loss 0.6387, train acc 78.090%, test acc 80.520%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.8246\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.1409\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 0.0940\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.3599\n",
      "Epoch: 006/030 | Batch 0500/0500 | Cost: 0.3031\n",
      "epoch 6, loss 0.5607, train acc 80.722%, test acc 83.240%\n",
      "EPOCH:  7 learning rate:  0.01\n",
      "Epoch: 007/030 | Batch 0100/0500 | Cost: 0.7060\n",
      "Epoch: 007/030 | Batch 0200/0500 | Cost: 0.4927\n",
      "Epoch: 007/030 | Batch 0300/0500 | Cost: 0.1303\n",
      "Epoch: 007/030 | Batch 0400/0500 | Cost: 0.1801\n",
      "Epoch: 007/030 | Batch 0500/0500 | Cost: 0.8755\n",
      "epoch 7, loss 0.5088, train acc 82.394%, test acc 82.210%\n",
      "EPOCH:  8 learning rate:  0.01\n",
      "Epoch: 008/030 | Batch 0100/0500 | Cost: 0.4062\n",
      "Epoch: 008/030 | Batch 0200/0500 | Cost: 0.8832\n",
      "Epoch: 008/030 | Batch 0300/0500 | Cost: 0.2879\n",
      "Epoch: 008/030 | Batch 0400/0500 | Cost: 0.5213\n",
      "Epoch: 008/030 | Batch 0500/0500 | Cost: 0.8005\n",
      "epoch 8, loss 0.4646, train acc 84.052%, test acc 85.420%\n",
      "EPOCH:  9 learning rate:  0.01\n",
      "Epoch: 009/030 | Batch 0100/0500 | Cost: 0.4458\n",
      "Epoch: 009/030 | Batch 0200/0500 | Cost: 0.3003\n",
      "Epoch: 009/030 | Batch 0300/0500 | Cost: 0.4265\n",
      "Epoch: 009/030 | Batch 0400/0500 | Cost: 0.4478\n",
      "Epoch: 009/030 | Batch 0500/0500 | Cost: 0.0732\n",
      "epoch 9, loss 0.4275, train acc 85.296%, test acc 86.060%\n",
      "EPOCH:  10 learning rate:  0.01\n",
      "Epoch: 010/030 | Batch 0100/0500 | Cost: 0.2563\n",
      "Epoch: 010/030 | Batch 0200/0500 | Cost: 0.4469\n",
      "Epoch: 010/030 | Batch 0300/0500 | Cost: 0.2433\n",
      "Epoch: 010/030 | Batch 0400/0500 | Cost: 0.6382\n",
      "Epoch: 010/030 | Batch 0500/0500 | Cost: 0.3886\n",
      "epoch 10, loss 0.3902, train acc 86.634%, test acc 86.880%\n",
      "EPOCH:  11 learning rate:  0.01\n",
      "Epoch: 011/030 | Batch 0100/0500 | Cost: 0.4885\n",
      "Epoch: 011/030 | Batch 0200/0500 | Cost: 0.2500\n",
      "Epoch: 011/030 | Batch 0300/0500 | Cost: 0.6056\n",
      "Epoch: 011/030 | Batch 0400/0500 | Cost: 0.5441\n",
      "Epoch: 011/030 | Batch 0500/0500 | Cost: 0.5523\n",
      "epoch 11, loss 0.3635, train acc 87.614%, test acc 85.330%\n",
      "EPOCH:  12 learning rate:  0.01\n",
      "Epoch: 012/030 | Batch 0100/0500 | Cost: 0.7015\n",
      "Epoch: 012/030 | Batch 0200/0500 | Cost: 0.1831\n",
      "Epoch: 012/030 | Batch 0300/0500 | Cost: 0.5253\n",
      "Epoch: 012/030 | Batch 0400/0500 | Cost: 0.3725\n",
      "Epoch: 012/030 | Batch 0500/0500 | Cost: 0.3392\n",
      "epoch 12, loss 0.3391, train acc 88.318%, test acc 88.350%\n",
      "EPOCH:  13 learning rate:  0.01\n",
      "Epoch: 013/030 | Batch 0100/0500 | Cost: 0.4028\n",
      "Epoch: 013/030 | Batch 0200/0500 | Cost: 0.6606\n",
      "Epoch: 013/030 | Batch 0300/0500 | Cost: 0.6965\n",
      "Epoch: 013/030 | Batch 0400/0500 | Cost: 0.7460\n",
      "Epoch: 013/030 | Batch 0500/0500 | Cost: 0.3785\n",
      "epoch 13, loss 0.3140, train acc 89.336%, test acc 87.740%\n",
      "EPOCH:  14 learning rate:  0.01\n",
      "Epoch: 014/030 | Batch 0100/0500 | Cost: 0.1001\n",
      "Epoch: 014/030 | Batch 0200/0500 | Cost: 0.4951\n",
      "Epoch: 014/030 | Batch 0300/0500 | Cost: 0.2639\n",
      "Epoch: 014/030 | Batch 0400/0500 | Cost: 0.1239\n",
      "Epoch: 014/030 | Batch 0500/0500 | Cost: 0.0905\n",
      "epoch 14, loss 0.2981, train acc 89.736%, test acc 87.260%\n",
      "EPOCH:  15 learning rate:  0.01\n",
      "Epoch: 015/030 | Batch 0100/0500 | Cost: 0.0386\n",
      "Epoch: 015/030 | Batch 0200/0500 | Cost: 0.6978\n",
      "Epoch: 015/030 | Batch 0300/0500 | Cost: 0.1025\n",
      "Epoch: 015/030 | Batch 0400/0500 | Cost: 0.2300\n",
      "Epoch: 015/030 | Batch 0500/0500 | Cost: 0.2030\n",
      "epoch 15, loss 0.2762, train acc 90.576%, test acc 88.500%\n",
      "EPOCH:  16 learning rate:  0.01\n",
      "Epoch: 016/030 | Batch 0100/0500 | Cost: 0.3804\n",
      "Epoch: 016/030 | Batch 0200/0500 | Cost: 0.2259\n",
      "Epoch: 016/030 | Batch 0300/0500 | Cost: 0.1526\n",
      "Epoch: 016/030 | Batch 0400/0500 | Cost: 0.0765\n",
      "Epoch: 016/030 | Batch 0500/0500 | Cost: 0.0255\n",
      "epoch 16, loss 0.2612, train acc 90.950%, test acc 89.170%\n",
      "EPOCH:  17 learning rate:  0.01\n",
      "Epoch: 017/030 | Batch 0100/0500 | Cost: 0.0335\n",
      "Epoch: 017/030 | Batch 0200/0500 | Cost: 0.1307\n",
      "Epoch: 017/030 | Batch 0300/0500 | Cost: 0.4022\n",
      "Epoch: 017/030 | Batch 0400/0500 | Cost: 0.4331\n",
      "Epoch: 017/030 | Batch 0500/0500 | Cost: 0.6998\n",
      "epoch 17, loss 0.2457, train acc 91.414%, test acc 85.600%\n",
      "EPOCH:  18 learning rate:  0.01\n",
      "Epoch: 018/030 | Batch 0100/0500 | Cost: 0.2106\n",
      "Epoch: 018/030 | Batch 0200/0500 | Cost: 0.1224\n",
      "Epoch: 018/030 | Batch 0300/0500 | Cost: 0.1387\n",
      "Epoch: 018/030 | Batch 0400/0500 | Cost: 0.5733\n",
      "Epoch: 018/030 | Batch 0500/0500 | Cost: 0.2574\n",
      "epoch 18, loss 0.2296, train acc 92.006%, test acc 89.200%\n",
      "EPOCH:  19 learning rate:  0.01\n",
      "Epoch: 019/030 | Batch 0100/0500 | Cost: 0.3095\n",
      "Epoch: 019/030 | Batch 0200/0500 | Cost: 0.2491\n",
      "Epoch: 019/030 | Batch 0300/0500 | Cost: 0.0975\n",
      "Epoch: 019/030 | Batch 0400/0500 | Cost: 0.4564\n",
      "Epoch: 019/030 | Batch 0500/0500 | Cost: 0.2388\n",
      "epoch 19, loss 0.2190, train acc 92.550%, test acc 89.060%\n",
      "EPOCH:  20 learning rate:  0.01\n",
      "Epoch: 020/030 | Batch 0100/0500 | Cost: 0.0568\n",
      "Epoch: 020/030 | Batch 0200/0500 | Cost: 0.1324\n",
      "Epoch: 020/030 | Batch 0300/0500 | Cost: 0.2830\n",
      "Epoch: 020/030 | Batch 0400/0500 | Cost: 0.2365\n",
      "Epoch: 020/030 | Batch 0500/0500 | Cost: 0.0899\n",
      "epoch 20, loss 0.2041, train acc 92.902%, test acc 90.000%\n",
      "EPOCH:  21 learning rate:  0.001\n",
      "Epoch: 021/030 | Batch 0100/0500 | Cost: 0.4415\n",
      "Epoch: 021/030 | Batch 0200/0500 | Cost: 0.1752\n",
      "Epoch: 021/030 | Batch 0300/0500 | Cost: 0.0367\n",
      "Epoch: 021/030 | Batch 0400/0500 | Cost: 0.0028\n",
      "****************************************************************************************************\n",
      "Iter_num: 10400 Test_acc 92.19 Comm_round: 104000\n",
      "****************************************************************************************************\n",
      "Epoch: 021/030 | Batch 0500/0500 | Cost: 0.0699\n",
      "epoch 21, loss 0.1359, train acc 95.350%, test acc 92.140%\n",
      "EPOCH:  22 learning rate:  0.001\n",
      "Epoch: 022/030 | Batch 0100/0500 | Cost: 0.0409\n",
      "Epoch: 022/030 | Batch 0200/0500 | Cost: 0.1510\n",
      "Epoch: 022/030 | Batch 0300/0500 | Cost: 0.0448\n",
      "Epoch: 022/030 | Batch 0400/0500 | Cost: 0.0120\n",
      "Epoch: 022/030 | Batch 0500/0500 | Cost: 0.0899\n",
      "epoch 22, loss 0.1193, train acc 95.988%, test acc 92.180%\n",
      "EPOCH:  23 learning rate:  0.001\n",
      "Epoch: 023/030 | Batch 0100/0500 | Cost: 0.0070\n",
      "Epoch: 023/030 | Batch 0200/0500 | Cost: 0.5955\n",
      "Epoch: 023/030 | Batch 0300/0500 | Cost: 0.0211\n",
      "Epoch: 023/030 | Batch 0400/0500 | Cost: 0.0125\n",
      "Epoch: 023/030 | Batch 0500/0500 | Cost: 0.3823\n",
      "epoch 23, loss 0.1068, train acc 96.458%, test acc 92.200%\n",
      "EPOCH:  24 learning rate:  0.001\n",
      "Epoch: 024/030 | Batch 0100/0500 | Cost: 0.0190\n",
      "Epoch: 024/030 | Batch 0200/0500 | Cost: 0.0120\n",
      "Epoch: 024/030 | Batch 0300/0500 | Cost: 0.0708\n",
      "Epoch: 024/030 | Batch 0400/0500 | Cost: 0.0065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024/030 | Batch 0500/0500 | Cost: 0.0179\n",
      "epoch 24, loss 0.1011, train acc 96.652%, test acc 92.360%\n",
      "EPOCH:  25 learning rate:  0.001\n",
      "Epoch: 025/030 | Batch 0100/0500 | Cost: 0.0828\n",
      "Epoch: 025/030 | Batch 0200/0500 | Cost: 0.0220\n",
      "Epoch: 025/030 | Batch 0300/0500 | Cost: 0.1611\n",
      "Epoch: 025/030 | Batch 0400/0500 | Cost: 0.1672\n",
      "Epoch: 025/030 | Batch 0500/0500 | Cost: 0.0590\n",
      "epoch 25, loss 0.0989, train acc 96.670%, test acc 92.180%\n",
      "EPOCH:  26 learning rate:  0.001\n",
      "Epoch: 026/030 | Batch 0100/0500 | Cost: 0.0106\n",
      "Epoch: 026/030 | Batch 0200/0500 | Cost: 0.0080\n",
      "Epoch: 026/030 | Batch 0300/0500 | Cost: 0.1520\n",
      "Epoch: 026/030 | Batch 0400/0500 | Cost: 0.1105\n",
      "Epoch: 026/030 | Batch 0500/0500 | Cost: 0.0141\n",
      "epoch 26, loss 0.0942, train acc 96.872%, test acc 92.330%\n",
      "EPOCH:  27 learning rate:  0.001\n",
      "Epoch: 027/030 | Batch 0100/0500 | Cost: 0.0351\n",
      "Epoch: 027/030 | Batch 0200/0500 | Cost: 0.0798\n",
      "Epoch: 027/030 | Batch 0300/0500 | Cost: 0.0681\n",
      "Epoch: 027/030 | Batch 0400/0500 | Cost: 0.0289\n",
      "Epoch: 027/030 | Batch 0500/0500 | Cost: 0.0039\n",
      "epoch 27, loss 0.0898, train acc 96.960%, test acc 92.350%\n",
      "EPOCH:  28 learning rate:  0.001\n",
      "Epoch: 028/030 | Batch 0100/0500 | Cost: 0.0055\n",
      "Epoch: 028/030 | Batch 0200/0500 | Cost: 0.1709\n",
      "Epoch: 028/030 | Batch 0300/0500 | Cost: 0.0063\n",
      "Epoch: 028/030 | Batch 0400/0500 | Cost: 0.3868\n",
      "Epoch: 028/030 | Batch 0500/0500 | Cost: 0.1201\n",
      "epoch 28, loss 0.0884, train acc 96.966%, test acc 92.240%\n",
      "EPOCH:  29 learning rate:  0.001\n",
      "Epoch: 029/030 | Batch 0100/0500 | Cost: 0.2466\n",
      "Epoch: 029/030 | Batch 0200/0500 | Cost: 0.0447\n",
      "Epoch: 029/030 | Batch 0300/0500 | Cost: 0.0096\n",
      "Epoch: 029/030 | Batch 0400/0500 | Cost: 0.0428\n",
      "Epoch: 029/030 | Batch 0500/0500 | Cost: 0.0237\n",
      "epoch 29, loss 0.0847, train acc 97.132%, test acc 92.250%\n",
      "EPOCH:  30 learning rate:  0.001\n",
      "Epoch: 030/030 | Batch 0100/0500 | Cost: 0.0499\n",
      "Epoch: 030/030 | Batch 0200/0500 | Cost: 0.0255\n",
      "Epoch: 030/030 | Batch 0300/0500 | Cost: 0.0052\n",
      "Epoch: 030/030 | Batch 0400/0500 | Cost: 0.0764\n",
      "Epoch: 030/030 | Batch 0500/0500 | Cost: 0.0040\n",
      "epoch 30, loss 0.0829, train acc 97.166%, test acc 92.490%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Dis-SGD  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "\n",
    "    # adjust learning_rate\n",
    "    lr, _= adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    grad_agg[p_id] += lr * p.grad.data.clone().detach()\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 10)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SGD-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SGD-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original communication number: 1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (60,) and (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[1;32m     48\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomm_round_dis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest-acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSGD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(comm_round_sparse, sparse_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparse\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m)\n\u001b[1;32m     51\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(comm_round_LASG, LASG_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLASG\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/pyplot.py:2748\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1668\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1668\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (60,) and (50,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record once every 100 iterations, and 10 workers need to communicate in each iteration\n",
    "original_num = 1000\n",
    "print(\"original communication number:\", original_num)\n",
    "\n",
    "TASGS_data = pd.read_csv(\"./result/\"+\"TASGS-res-cifar10-iter-84.csv\")\n",
    "SASG_data = pd.read_csv(\"./result/\"+\"SASG-res-cifar10-iter-84.csv\")\n",
    "LASG_data = pd.read_csv(\"./result/\"+\"LASG-res-cifar10-iter-84.csv\")\n",
    "sparse_data = pd.read_csv(\"./result/\"+\"Sparse-res-cifar10-iter-84.csv\")\n",
    "sgd_data = pd.read_csv(\"./result/\"+\"SGD-res-cifar10-iter-84.csv\")\n",
    "\n",
    "TASGS_skip = TASGS_data['Skip'].values.tolist()\n",
    "SASG_skip = SASG_data['Skip'].values.tolist()\n",
    "LASG_skip = LASG_data['Skip'].values.tolist()\n",
    "\n",
    "\n",
    "comm_round_TASGS = []\n",
    "comm_round_SASG = []\n",
    "comm_round_LASG = []\n",
    "comm_round_sparse = []\n",
    "comm_round_dis = []\n",
    "\n",
    "comm_bit_TASGS = []\n",
    "comm_bit_SASG = []\n",
    "comm_bit_LASG = []\n",
    "comm_bit_sparse = []\n",
    "comm_bit_dis = []\n",
    "\n",
    "comm_num_TASGS, comm_num_SASG, comm_num_LASG, comm_num_sparse, comm_num_dis = 0, 0, 0, 0, 0\n",
    "for i in range(len(TASGS_skip)):\n",
    "    comm_num_TASGS += original_num\n",
    "    comm_num_SASG += original_num\n",
    "    comm_num_LASG += original_num\n",
    "    comm_num_sparse += original_num\n",
    "    comm_num_dis += original_num\n",
    "    \n",
    "    comm_round_TASGS.append(comm_num_TASGS - TASGS_skip[i])\n",
    "    comm_round_SASG.append(comm_num_SASG - SASG_skip[i])\n",
    "    comm_round_LASG.append(comm_num_LASG - LASG_skip[i])\n",
    "    comm_round_sparse.append(comm_num_sparse)\n",
    "    comm_round_dis.append(comm_num_dis)\n",
    "\n",
    "font1 = {'weight': 'normal', 'size': 17}\n",
    "font2 = {'weight': 'normal', 'size': 20}\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='both', linewidth=1.2)\n",
    "plt.plot(comm_round_dis, sgd_data['test-acc'].values.tolist(), 'black', label='SGD', linewidth=2.5)\n",
    "plt.plot(comm_round_sparse, sparse_data['test-acc'].values.tolist(), 'b', label='Sparse', linewidth=2.5)\n",
    "plt.plot(comm_round_LASG, LASG_data['test-acc'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "plt.plot(comm_round_SASG, SASG_data['test-acc'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "plt.plot(comm_round_TASGS, TASGS_data['test-acc'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(50000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(25000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.ylim(30, 95)\n",
    "ymajorLocator = MultipleLocator(15)  # major\n",
    "ax.yaxis.set_major_locator(ymajorLocator)\n",
    "yminorLocator = MultipleLocator(15)  # minor\n",
    "ax.yaxis.set_minor_locator(yminorLocator)\n",
    "plt.ylabel('Test Accuracy', font2)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.13, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"test_res_cifar10.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_dis, sgd_data['Loss'].values.tolist(), 'black', label='SGD', linewidth=2.5)\n",
    "plt.plot(comm_round_sparse, sparse_data['Loss'].values.tolist(), 'b', label='Sparse', linewidth=2.5)\n",
    "plt.plot(comm_round_LASG, LASG_data['Loss'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "plt.plot(comm_round_SASG, SASG_data['Loss'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "plt.plot(comm_round_TASGS, TASGS_data['Loss'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(50000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(25000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss', font2)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.16, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"loss_res_cifar10.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
