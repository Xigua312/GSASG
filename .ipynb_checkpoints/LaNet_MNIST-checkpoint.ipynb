{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Result for **FullConnect+MNIST**\n",
    "### This notebook is for seed $42$. The results in the paper are the average of $5$ times with seeds in $[1, 19, 31, 42, 80]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from opacus import PrivacyEngine\n",
    "# from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "# import skimage\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MNIST Fully Connected Net ----------------------------------------------------\n",
    "# class MNISTNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MNISTNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(28 * 28, 512)\n",
    "#         self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # flatten image input\n",
    "#         x = x.view(-1, 28 * 28)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class MNISTNet_copy(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MNISTNet_copy, self).__init__()\n",
    "#         self.fc1 = nn.Linear(28 * 28, 512)\n",
    "#         self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # flatten image input\n",
    "#         x = x.view(-1, 28 * 28)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, \n",
    "                           kernel_size = 4, stride = 1, padding = 0) #C1\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, \n",
    "                           kernel_size = 4, stride = 1, padding = 0) #C3\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 120, \n",
    "                           kernel_size = 4, stride = 1, padding = 0) #C5\n",
    "        self.linear1 = nn.Linear(120, 84) # F6\n",
    "        self.linear2 = nn.Linear(84, 10) # output\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.avgpool(x) #S2\n",
    "        x = self.conv2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.avgpool(x) #S4\n",
    "        x = self.conv3(x)\n",
    "        x = self.tanh(x)\n",
    "    \n",
    "        x = x.reshape(x.shape[0], -1) \n",
    "        x = self.linear1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# class LeNet_copy(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LeNet_copy, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, \n",
    "#                            kernel_size = 5, stride = 1, padding = 0) #C1\n",
    "#         self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, \n",
    "#                            kernel_size = 5, stride = 1, padding = 0) #C3\n",
    "#         self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 120, \n",
    "#                            kernel_size = 5, stride = 1, padding = 0) #C5\n",
    "#         self.linear1 = nn.Linear(120, 84) # F6\n",
    "#         self.linear2 = nn.Linear(84, 10) # output\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.avgpool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.tanh(x)\n",
    "#         x = self.avgpool(x) #S2\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.tanh(x)\n",
    "#         x = self.avgpool(x) #S4\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.tanh(x)\n",
    "    \n",
    "#         x = x.reshape(x.shape[0], -1) \n",
    "#         x = self.linear1(x)\n",
    "#         x = self.tanh(x)\n",
    "#         x = self.linear2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_loaders(dataset_name, n_workers, batch_size):\n",
    "    train_data, test_data = load_data(dataset_name)\n",
    "    train_loader_workers = dict()\n",
    "    n = len(train_data)\n",
    "    \n",
    "    # preparing iterators for workers\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    a = np.int64(np.floor(n / n_workers))\n",
    "    top_ind = a * n_workers\n",
    "    seq = range(a, top_ind, a)\n",
    "    split = np.split(indices[:top_ind], seq)\n",
    "    b = 0\n",
    "    for ind in split:\n",
    "        train_loader_workers[b] = DataLoader(Subset(train_data, ind), batch_size=batch_size, shuffle=True)\n",
    "        b = b + 1\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_workers, test_loader\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'mnist':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        train_data = datasets.MNIST(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "        test_data = datasets.MNIST(root='./data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "    elif dataset_name == 'fashionmnist':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        train_data = datasets.FashionMNIST(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "        test_data = datasets.FashionMNIST(root='./data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "\n",
    "\n",
    "    elif dataset_name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        train_data = datasets.CIFAR10(root='./data', train=True,\n",
    "                                      download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR10(root='./data', train=False,\n",
    "                                     download=True, transform=transform_test)\n",
    "\n",
    "    elif dataset_name == 'cifar100':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        train_data = datasets.CIFAR100(root='./data', train=True,\n",
    "                                       download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR100(root='./data', train=False,\n",
    "                                      download=True, transform=transform_test)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(dataset_name + ' is not known.')\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parameter and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "lr = 0.1\n",
    "BATCH_SIZE = 30\n",
    "NUM_EPOCHS = 10\n",
    "NUM_WORKERS = 4\n",
    "mean =0\n",
    "var= 0.00001\n",
    "# D = 10  # compute thread\n",
    "# h = 0.01  # sparse level 99%\n",
    "# h1= 0.005 # gsparse level\n",
    "\n",
    "\n",
    "# mean = 0\n",
    "# var = 0.05\n",
    "#var表示全局敏感度*delta，全局敏感=1，delta=1.4，c=1\n",
    "# RGB\n",
    "# noise = sigma * np.random.randn(256, 256, 3)#高斯噪声\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \n",
    "def evaluate_accuracy(model, data_iter, device):\n",
    "    acc_sum, num_epo = 0.0, 0\n",
    "    for image, label in data_iter:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        acc_sum += (model(image).argmax(dim=1) == label).float().sum().item()\n",
    "        num_epo += label.shape[0]\n",
    "    return acc_sum / num_epo * 100\n",
    "\n",
    "\n",
    "# 定义Top_k sparse\n",
    "def prep_grad(x):\n",
    "    x_flat = torch.unsqueeze(x, 0).flatten()\n",
    "    dim = x.shape#dim代表维度，相当于有多少个数\n",
    "    d = x_flat.shape[0]\n",
    "    return x_flat, dim, d\n",
    "\n",
    "\n",
    "def top_k_opt(x, h):\n",
    "    \"\"\"\n",
    "    :param x: vector to sparsify\n",
    "    :param h: density\n",
    "    :return: compressed vector\n",
    "    \"\"\"\n",
    "    x, dim, d = prep_grad(x)\n",
    "    # number of coordinates kept保留的坐标数，保留top多少 r就是这个k的数量\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    # positions of top_k coordinates topk的坐标位置，保留了值和索引\n",
    "    _, ind = torch.topk(torch.abs(x), r)\n",
    "    mask = torch.zeros_like(x)#生成与x相同的全0张量\n",
    "    mask[ind] = 1#把索引那一行全置为1\n",
    "    t = mask * x\n",
    "    #上面是top-k\n",
    "    t = t.reshape(dim)\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "def compt(old, new):\n",
    "    result = 0\n",
    "    for i in range(len(old)):\n",
    "        result += ((old[i].view(-1) - new[i].view(-1)) ** 2).sum().item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GTOP-K-SASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSASG  lr:0.005--h:0.01--epoch:20--worker:4\n",
      "Number_parameter: 4\n",
      "Element_parameter: 407050\n",
      "Element_parameter_sparse: 4071\n",
      "Element_parameter_sparse_gtop: 4071\n",
      "Epoch: 001/020 | Batch 0100/0300 | Cost: 98134128.0000\n",
      "Epoch: 001/020 | Batch 0200/0300 | Cost: 54769080.0000\n",
      "Epoch: 001/020 | Batch 0300/0300 | Cost: 25190512.0000\n",
      "epoch 1, skip_num 104, loss 3443075127.6088, train acc 60.880%, test acc 74.620%\n",
      "Epoch: 002/020 | Batch 0100/0300 | Cost: 8172013.0000\n",
      "Epoch: 002/020 | Batch 0200/0300 | Cost: 23951778.0000\n",
      "Epoch: 002/020 | Batch 0300/0300 | Cost: 5121054.5000\n",
      "epoch 2, skip_num 65, loss 48895666.4327, train acc 76.137%, test acc 77.310%\n",
      "Epoch: 003/020 | Batch 0100/0300 | Cost: 4133244.2500\n",
      "Epoch: 003/020 | Batch 0200/0300 | Cost: 7368881.5000\n",
      "Epoch: 003/020 | Batch 0300/0300 | Cost: 14857597.0000\n",
      "epoch 3, skip_num 87, loss 29427999.5778, train acc 78.067%, test acc 78.600%\n",
      "Epoch: 004/020 | Batch 0100/0300 | Cost: 6657875.0000\n",
      "Epoch: 004/020 | Batch 0200/0300 | Cost: 10301011.0000\n",
      "Epoch: 004/020 | Batch 0300/0300 | Cost: 5621026.5000\n",
      "epoch 4, skip_num 82, loss 21070404.9677, train acc 78.848%, test acc 78.210%\n",
      "Epoch: 005/020 | Batch 0100/0300 | Cost: 11925614.0000\n",
      "Epoch: 005/020 | Batch 0200/0300 | Cost: 21901806.0000\n",
      "Epoch: 005/020 | Batch 0300/0300 | Cost: 6662706.5000\n",
      "epoch 5, skip_num 172, loss 16254301.6770, train acc 79.933%, test acc 78.170%\n",
      "Epoch: 006/020 | Batch 0100/0300 | Cost: 71855568.0000\n",
      "Epoch: 006/020 | Batch 0200/0300 | Cost: 2436383.2500\n",
      "Epoch: 006/020 | Batch 0300/0300 | Cost: 5007989.0000\n",
      "epoch 6, skip_num 210, loss 13046073.3152, train acc 80.070%, test acc 80.620%\n",
      "Epoch: 007/020 | Batch 0100/0300 | Cost: 545743.9375\n",
      "Epoch: 007/020 | Batch 0200/0300 | Cost: 2478888.2500\n",
      "Epoch: 007/020 | Batch 0300/0300 | Cost: 3613083.5000\n",
      "epoch 7, skip_num 269, loss 10760821.1401, train acc 80.680%, test acc 80.120%\n",
      "Epoch: 008/020 | Batch 0100/0300 | Cost: 326493.1250\n",
      "Epoch: 008/020 | Batch 0200/0300 | Cost: 1250598.3750\n",
      "Epoch: 008/020 | Batch 0300/0300 | Cost: 391925.5625\n",
      "epoch 8, skip_num 363, loss 9217739.8350, train acc 81.017%, test acc 80.070%\n",
      "Epoch: 009/020 | Batch 0100/0300 | Cost: 1912135.2500\n",
      "Epoch: 009/020 | Batch 0200/0300 | Cost: 4597367.0000\n",
      "Epoch: 009/020 | Batch 0300/0300 | Cost: 6794505.0000\n",
      "epoch 9, skip_num 401, loss 7995447.2526, train acc 80.882%, test acc 75.710%\n",
      "Epoch: 010/020 | Batch 0100/0300 | Cost: 61025.7969\n",
      "Epoch: 010/020 | Batch 0200/0300 | Cost: 1936234.8750\n",
      "Epoch: 010/020 | Batch 0300/0300 | Cost: 1417381.6250\n",
      "epoch 10, skip_num 479, loss 6973093.4957, train acc 80.668%, test acc 77.480%\n",
      "Epoch: 011/020 | Batch 0100/0300 | Cost: 669755.8125\n",
      "Epoch: 011/020 | Batch 0200/0300 | Cost: 104302.0234\n",
      "Epoch: 011/020 | Batch 0300/0300 | Cost: 39877664.0000\n",
      "epoch 11, skip_num 527, loss 6112004.5753, train acc 80.612%, test acc 76.420%\n",
      "Epoch: 012/020 | Batch 0100/0300 | Cost: 3242291.2500\n",
      "Epoch: 012/020 | Batch 0200/0300 | Cost: 2431787.0000\n",
      "Epoch: 012/020 | Batch 0300/0300 | Cost: 250661.7344\n",
      "epoch 12, skip_num 590, loss 5515768.9335, train acc 79.920%, test acc 74.960%\n",
      "Epoch: 013/020 | Batch 0100/0300 | Cost: 2898188.7500\n",
      "Epoch: 013/020 | Batch 0200/0300 | Cost: 212290.2344\n",
      "Epoch: 013/020 | Batch 0300/0300 | Cost: 809713.1250\n",
      "epoch 13, skip_num 615, loss 4913159.0361, train acc 77.155%, test acc 73.230%\n",
      "Epoch: 014/020 | Batch 0100/0300 | Cost: 789841.4375\n",
      "Epoch: 014/020 | Batch 0200/0300 | Cost: 3561766.0000\n",
      "Epoch: 014/020 | Batch 0300/0300 | Cost: 2426987.7500\n",
      "epoch 14, skip_num 653, loss 4505080.5568, train acc 73.807%, test acc 74.360%\n",
      "Epoch: 015/020 | Batch 0100/0300 | Cost: 20146642.0000\n",
      "Epoch: 015/020 | Batch 0200/0300 | Cost: 3985139.2500\n",
      "Epoch: 015/020 | Batch 0300/0300 | Cost: 1245370.3750\n",
      "epoch 15, skip_num 691, loss 4160124.7365, train acc 72.787%, test acc 68.970%\n",
      "Epoch: 016/020 | Batch 0100/0300 | Cost: 858662.6250\n",
      "Epoch: 016/020 | Batch 0200/0300 | Cost: 758928.1875\n",
      "Epoch: 016/020 | Batch 0300/0300 | Cost: 288897.6875\n",
      "epoch 16, skip_num 755, loss 3703210.9651, train acc 71.807%, test acc 70.300%\n",
      "Epoch: 017/020 | Batch 0100/0300 | Cost: 60806.4062\n",
      "Epoch: 017/020 | Batch 0200/0300 | Cost: 1230477.5000\n",
      "Epoch: 017/020 | Batch 0300/0300 | Cost: 330353.0312\n",
      "epoch 17, skip_num 758, loss 3511248.3490, train acc 69.823%, test acc 67.720%\n",
      "Epoch: 018/020 | Batch 0100/0300 | Cost: 169189.4219\n",
      "Epoch: 018/020 | Batch 0200/0300 | Cost: 4756670.5000\n",
      "Epoch: 018/020 | Batch 0300/0300 | Cost: 10566.4072\n",
      "epoch 18, skip_num 771, loss 3258640.4983, train acc 69.243%, test acc 66.790%\n",
      "Epoch: 019/020 | Batch 0100/0300 | Cost: 50990.9258\n",
      "Epoch: 019/020 | Batch 0200/0300 | Cost: 16661354.0000\n",
      "Epoch: 019/020 | Batch 0300/0300 | Cost: 1042336.6250\n",
      "epoch 19, skip_num 808, loss 2939303.5303, train acc 69.107%, test acc 66.860%\n",
      "Epoch: 020/020 | Batch 0100/0300 | Cost: 18497502.0000\n",
      "Epoch: 020/020 | Batch 0200/0300 | Cost: 3095475.5000\n",
      "Epoch: 020/020 | Batch 0300/0300 | Cost: 1340335.1250\n"
     ]
    }
   ],
   "source": [
    "print(\"GSASG  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "model_copy = MNISTNet_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 计算损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "total_params_sparse_gtop = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)#\n",
    "    r = int(np.maximum(1, np.floor(d * h)))#h是代表稀疏化程度百分之99，floor向下取整\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)#稀疏化之后参与计算的参数也就是参数数量\n",
    "\n",
    "for p in model.parameters():\n",
    "    u = top_k_opt(p, h)\n",
    "    x1, dim1, d1 = prep_grad(u)#\n",
    "    s = int(np.maximum(1, np.floor(d1 * h)))#h是代表稀疏化程度百分之99，floor向下取整\n",
    "    total_params_sparse_gtop += s\n",
    "print(\"Element_parameter_sparse_gtop:\", total_params_sparse_gtop)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行\n",
    "grad_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行  新加的！！！！\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]# 新加的！！！！\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []#跳过的轮次\n",
    "Loss_epoch = []#损失的轮次\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "Round_epoch = []\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "L=0\n",
    "H=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "   \n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "      \n",
    "            for w_id in range(NUM_WORKERS):\n",
    "#                 print(NUM_WORKERS)\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():#每个work的本地训练循环\n",
    "#                         size_in_bytes = sys.getsizeof(model.parameters())#计算一个参数的bit\n",
    "#                         size_in_bits = size_in_bytes * 8\n",
    "#                         print(size_in_bits)\n",
    "                        g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)#本地top-k稀疏化\n",
    "                        Tk_sparse_clip = clip_grad_norm_(Tk_sparse, max_norm=1.0, norm_type=2) #梯度修剪\n",
    "                        noise = np.random.normal(mean, var**2, Tk_sparse_clip.shape)#计算噪声大小\n",
    "                        Tk_sparse_clip_noise = Tk_sparse_clip + noise\n",
    "#                         Tk_sparse_noise_clip = clip_grad_norm_(Tk_sparse_clip_noise, max_norm=20, norm_type=2)\n",
    "                        grad_agg[p_id] += Tk_sparse_clip_noise\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse_clip_noise\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse_clip_noise\n",
    "                        GTk_sparse = top_k_opt(grad_worker[w_id][p_id], h1)#111111\n",
    "                        grad_gtagg[p_id] += GTk_sparse#11111111\n",
    "                        error_gtworker[w_id][p_id] = error_worker[w_id][p_id] + (Tk_sparse - GTk_sparse)#1111111\n",
    "                        grad_gtworker[w_id][p_id] = GTk_sparse#1111111\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "                \n",
    "        else:\n",
    "      \n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (2 * lr * (NUM_WORKERS ** 2))\n",
    "\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    " \n",
    "                \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "                    \n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)#本地top-k稀疏化\n",
    "#                             Tk_sparse_clip = clip_grad_norm_(Tk_sparse, max_norm=1.0, norm_type=2) #梯度修剪\n",
    "#                             noise = np.random.normal(mean, var**2, Tk_sparse.shape)#计算噪声大小\n",
    "#                             Tk_sparse_clip_noise = Tk_sparse_clip + noise\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                            GTk_sparse = top_k_opt(grad_worker[w_id][p_id], h1)#1111112\n",
    "                            grad_gtagg[p_id] += GTk_sparse#111111112\n",
    "                            error_gtworker[w_id][p_id] = error_worker[w_id][p_id] + (Tk_sparse - GTk_sparse)#11111112\n",
    "                            grad_gtworker[w_id][p_id] = GTk_sparse#11111112\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]#111111\n",
    "                            grad_gtagg[p_id] += grad_gtworker[w_id][p_id]#11111111\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)#111111\n",
    "                p.data.add_(grad_gtagg[p_id], alpha=-1)#111111\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "                \n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "  \n",
    "                     \n",
    "#                      Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "                   \n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "# #                 Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "# save_csv\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-full-fa-noise1.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-full-iter-fa-nosie1.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASG  lr:0.005--h:0.01--epoch:20--worker:4\n",
      "Number_parameter: 4\n",
      "Element_parameter: 407050\n",
      "Element_parameter_sparse: 4071\n",
      "Epoch: 001/020 | Batch 0100/0300 | Cost: 0.6574\n",
      "Epoch: 001/020 | Batch 0200/0300 | Cost: 0.7872\n",
      "Epoch: 001/020 | Batch 0300/0300 | Cost: 0.8381\n",
      "epoch 1, skip_num 364, loss 0.8043, train acc 74.045%, test acc 80.340%\n",
      "Epoch: 002/020 | Batch 0100/0300 | Cost: 0.4900\n",
      "Epoch: 002/020 | Batch 0200/0300 | Cost: 0.5753\n",
      "Epoch: 002/020 | Batch 0300/0300 | Cost: 0.5593\n",
      "epoch 2, skip_num 105, loss 0.5086, train acc 82.433%, test acc 82.350%\n",
      "Epoch: 003/020 | Batch 0100/0300 | Cost: 0.3360\n",
      "Epoch: 003/020 | Batch 0200/0300 | Cost: 0.5666\n",
      "Epoch: 003/020 | Batch 0300/0300 | Cost: 0.4014\n",
      "epoch 3, skip_num 157, loss 0.4599, train acc 84.048%, test acc 83.240%\n",
      "Epoch: 004/020 | Batch 0100/0300 | Cost: 0.3929\n",
      "Epoch: 004/020 | Batch 0200/0300 | Cost: 0.3266\n",
      "Epoch: 004/020 | Batch 0300/0300 | Cost: 0.5338\n",
      "epoch 4, skip_num 122, loss 0.4328, train acc 84.967%, test acc 83.990%\n",
      "Epoch: 005/020 | Batch 0100/0300 | Cost: 0.4018\n",
      "Epoch: 005/020 | Batch 0200/0300 | Cost: 0.5270\n",
      "Epoch: 005/020 | Batch 0300/0300 | Cost: 0.4952\n",
      "epoch 5, skip_num 120, loss 0.4130, train acc 85.753%, test acc 84.210%\n",
      "Epoch: 006/020 | Batch 0100/0300 | Cost: 0.5266\n",
      "Epoch: 006/020 | Batch 0200/0300 | Cost: 0.3722\n",
      "Epoch: 006/020 | Batch 0300/0300 | Cost: 0.4893\n",
      "epoch 6, skip_num 119, loss 0.3979, train acc 86.053%, test acc 84.830%\n",
      "Epoch: 007/020 | Batch 0100/0300 | Cost: 0.2903\n",
      "Epoch: 007/020 | Batch 0200/0300 | Cost: 0.3071\n",
      "Epoch: 007/020 | Batch 0300/0300 | Cost: 0.4626\n",
      "epoch 7, skip_num 153, loss 0.3838, train acc 86.593%, test acc 85.160%\n",
      "Epoch: 008/020 | Batch 0100/0300 | Cost: 0.3993\n",
      "Epoch: 008/020 | Batch 0200/0300 | Cost: 0.4338\n",
      "Epoch: 008/020 | Batch 0300/0300 | Cost: 0.4131\n",
      "epoch 8, skip_num 133, loss 0.3733, train acc 86.875%, test acc 85.510%\n",
      "Epoch: 009/020 | Batch 0100/0300 | Cost: 0.3555\n",
      "Epoch: 009/020 | Batch 0200/0300 | Cost: 0.4927\n",
      "Epoch: 009/020 | Batch 0300/0300 | Cost: 0.2552\n",
      "epoch 9, skip_num 166, loss 0.3630, train acc 87.215%, test acc 85.270%\n",
      "Epoch: 010/020 | Batch 0100/0300 | Cost: 0.3550\n",
      "Epoch: 010/020 | Batch 0200/0300 | Cost: 0.5815\n",
      "Epoch: 010/020 | Batch 0300/0300 | Cost: 0.2955\n",
      "epoch 10, skip_num 160, loss 0.3547, train acc 87.463%, test acc 85.920%\n",
      "Epoch: 011/020 | Batch 0100/0300 | Cost: 0.2588\n",
      "Epoch: 011/020 | Batch 0200/0300 | Cost: 0.2386\n",
      "Epoch: 011/020 | Batch 0300/0300 | Cost: 0.3900\n",
      "****************************************************************************************************\n",
      "Iter_num: 3300 Test_acc 86.19 Skip_round: 1772 Comm_round: 11428\n",
      "****************************************************************************************************\n",
      "epoch 11, skip_num 173, loss 0.3469, train acc 87.852%, test acc 86.190%\n",
      "Epoch: 012/020 | Batch 0100/0300 | Cost: 0.3452\n",
      "Epoch: 012/020 | Batch 0200/0300 | Cost: 0.2465\n",
      "Epoch: 012/020 | Batch 0300/0300 | Cost: 0.3741\n",
      "epoch 12, skip_num 140, loss 0.3396, train acc 88.047%, test acc 86.030%\n",
      "Epoch: 013/020 | Batch 0100/0300 | Cost: 0.3560\n",
      "Epoch: 013/020 | Batch 0200/0300 | Cost: 0.2675\n",
      "Epoch: 013/020 | Batch 0300/0300 | Cost: 0.4003\n",
      "epoch 13, skip_num 162, loss 0.3324, train acc 88.312%, test acc 86.490%\n",
      "Epoch: 014/020 | Batch 0100/0300 | Cost: 0.5154\n",
      "Epoch: 014/020 | Batch 0200/0300 | Cost: 0.3767\n",
      "Epoch: 014/020 | Batch 0300/0300 | Cost: 0.3486\n",
      "epoch 14, skip_num 145, loss 0.3266, train acc 88.498%, test acc 86.760%\n",
      "Epoch: 015/020 | Batch 0100/0300 | Cost: 0.4272\n",
      "Epoch: 015/020 | Batch 0200/0300 | Cost: 0.5222\n",
      "Epoch: 015/020 | Batch 0300/0300 | Cost: 0.3912\n",
      "epoch 15, skip_num 164, loss 0.3204, train acc 88.632%, test acc 86.860%\n",
      "Epoch: 016/020 | Batch 0100/0300 | Cost: 0.3125\n",
      "Epoch: 016/020 | Batch 0200/0300 | Cost: 0.2492\n",
      "Epoch: 016/020 | Batch 0300/0300 | Cost: 0.2926\n",
      "epoch 16, skip_num 163, loss 0.3153, train acc 88.848%, test acc 86.760%\n",
      "Epoch: 017/020 | Batch 0100/0300 | Cost: 0.0973\n",
      "Epoch: 017/020 | Batch 0200/0300 | Cost: 0.1659\n",
      "Epoch: 017/020 | Batch 0300/0300 | Cost: 0.2483\n",
      "epoch 17, skip_num 149, loss 0.3097, train acc 89.028%, test acc 87.230%\n",
      "Epoch: 018/020 | Batch 0100/0300 | Cost: 0.3082\n",
      "Epoch: 018/020 | Batch 0200/0300 | Cost: 0.4036\n",
      "Epoch: 018/020 | Batch 0300/0300 | Cost: 0.4228\n",
      "epoch 18, skip_num 149, loss 0.3051, train acc 89.207%, test acc 86.480%\n",
      "Epoch: 019/020 | Batch 0100/0300 | Cost: 0.4602\n",
      "Epoch: 019/020 | Batch 0200/0300 | Cost: 0.4800\n",
      "Epoch: 019/020 | Batch 0300/0300 | Cost: 0.3867\n",
      "epoch 19, skip_num 112, loss 0.3011, train acc 89.307%, test acc 87.010%\n",
      "Epoch: 020/020 | Batch 0100/0300 | Cost: 0.2196\n",
      "Epoch: 020/020 | Batch 0200/0300 | Cost: 0.3299\n",
      "Epoch: 020/020 | Batch 0300/0300 | Cost: 0.2794\n",
      "epoch 20, skip_num 139, loss 0.2967, train acc 89.480%, test acc 87.530%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"SASG  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "model_copy = MNISTNet_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Round_epoch = []\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)\n",
    "                        grad_agg[p_id] += Tk_sparse\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (2 * lr * (NUM_WORKERS ** 2))\n",
    "\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "\n",
    "                \n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "                \n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "\n",
    "\n",
    "# #                 flag_acc = True\n",
    "# #                 print(\"*\" * 100)\n",
    "# #                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "# #                 print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append(iter_num* 4 - skip_iter)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "#             print(Round_epoch)\n",
    "                \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "comm_iter = iter_num * 4 - skip_iter\n",
    "# save_csv\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SASG-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SASG-full-iter-fa.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# LASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASG  lr:0.005--epoch:20--worker:4\n",
      "Number_parameter: 4\n",
      "Element_parameter: 407050\n",
      "Epoch: 001/020 | Batch 0100/0300 | Cost: 0.6440\n",
      "Epoch: 001/020 | Batch 0200/0300 | Cost: 0.7921\n",
      "Epoch: 001/020 | Batch 0300/0300 | Cost: 0.8146\n",
      "epoch 1, skip_num 0, loss 0.7441, train acc 76.422%, test acc 80.070%\n",
      "Epoch: 002/020 | Batch 0100/0300 | Cost: 0.4822\n",
      "Epoch: 002/020 | Batch 0200/0300 | Cost: 0.5707\n",
      "Epoch: 002/020 | Batch 0300/0300 | Cost: 0.5466\n",
      "epoch 2, skip_num 0, loss 0.5064, train acc 82.593%, test acc 82.500%\n",
      "Epoch: 003/020 | Batch 0100/0300 | Cost: 0.3772\n",
      "Epoch: 003/020 | Batch 0200/0300 | Cost: 0.5579\n",
      "Epoch: 003/020 | Batch 0300/0300 | Cost: 0.4002\n",
      "epoch 3, skip_num 0, loss 0.4592, train acc 84.080%, test acc 83.430%\n",
      "Epoch: 004/020 | Batch 0100/0300 | Cost: 0.3921\n",
      "Epoch: 004/020 | Batch 0200/0300 | Cost: 0.3010\n",
      "Epoch: 004/020 | Batch 0300/0300 | Cost: 0.6485\n",
      "epoch 4, skip_num 0, loss 0.4333, train acc 84.880%, test acc 83.600%\n",
      "Epoch: 005/020 | Batch 0100/0300 | Cost: 0.3970\n",
      "Epoch: 005/020 | Batch 0200/0300 | Cost: 0.4986\n",
      "Epoch: 005/020 | Batch 0300/0300 | Cost: 0.4939\n",
      "epoch 5, skip_num 0, loss 0.4130, train acc 85.527%, test acc 83.260%\n",
      "Epoch: 006/020 | Batch 0100/0300 | Cost: 0.5352\n",
      "Epoch: 006/020 | Batch 0200/0300 | Cost: 0.3642\n",
      "Epoch: 006/020 | Batch 0300/0300 | Cost: 0.4824\n",
      "epoch 6, skip_num 0, loss 0.3975, train acc 86.005%, test acc 84.950%\n",
      "Epoch: 007/020 | Batch 0100/0300 | Cost: 0.2857\n",
      "Epoch: 007/020 | Batch 0200/0300 | Cost: 0.3158\n",
      "Epoch: 007/020 | Batch 0300/0300 | Cost: 0.4406\n",
      "epoch 7, skip_num 0, loss 0.3827, train acc 86.678%, test acc 85.130%\n",
      "Epoch: 008/020 | Batch 0100/0300 | Cost: 0.3692\n",
      "Epoch: 008/020 | Batch 0200/0300 | Cost: 0.4133\n",
      "Epoch: 008/020 | Batch 0300/0300 | Cost: 0.3962\n",
      "epoch 8, skip_num 0, loss 0.3720, train acc 86.947%, test acc 85.410%\n",
      "Epoch: 009/020 | Batch 0100/0300 | Cost: 0.3877\n",
      "Epoch: 009/020 | Batch 0200/0300 | Cost: 0.4974\n",
      "Epoch: 009/020 | Batch 0300/0300 | Cost: 0.2590\n",
      "epoch 9, skip_num 0, loss 0.3630, train acc 87.108%, test acc 85.510%\n",
      "Epoch: 010/020 | Batch 0100/0300 | Cost: 0.3266\n",
      "****************************************************************************************************\n",
      "Iter_num: 2800 Test_acc 86.02 Skip_round: 0 Comm_round: 11200\n",
      "****************************************************************************************************\n",
      "Epoch: 010/020 | Batch 0200/0300 | Cost: 0.5789\n",
      "Epoch: 010/020 | Batch 0300/0300 | Cost: 0.3012\n",
      "epoch 10, skip_num 0, loss 0.3534, train acc 87.440%, test acc 86.020%\n",
      "Epoch: 011/020 | Batch 0100/0300 | Cost: 0.2593\n",
      "Epoch: 011/020 | Batch 0200/0300 | Cost: 0.2495\n",
      "Epoch: 011/020 | Batch 0300/0300 | Cost: 0.3598\n",
      "epoch 11, skip_num 0, loss 0.3473, train acc 87.787%, test acc 86.220%\n",
      "Epoch: 012/020 | Batch 0100/0300 | Cost: 0.3333\n",
      "Epoch: 012/020 | Batch 0200/0300 | Cost: 0.2389\n",
      "Epoch: 012/020 | Batch 0300/0300 | Cost: 0.3634\n",
      "epoch 12, skip_num 0, loss 0.3390, train acc 88.023%, test acc 85.880%\n",
      "Epoch: 013/020 | Batch 0100/0300 | Cost: 0.3809\n",
      "Epoch: 013/020 | Batch 0200/0300 | Cost: 0.2757\n",
      "Epoch: 013/020 | Batch 0300/0300 | Cost: 0.4085\n",
      "epoch 13, skip_num 0, loss 0.3335, train acc 88.105%, test acc 86.210%\n",
      "Epoch: 014/020 | Batch 0100/0300 | Cost: 0.4943\n",
      "Epoch: 014/020 | Batch 0200/0300 | Cost: 0.3595\n",
      "Epoch: 014/020 | Batch 0300/0300 | Cost: 0.4252\n",
      "epoch 14, skip_num 0, loss 0.3263, train acc 88.408%, test acc 85.870%\n",
      "Epoch: 015/020 | Batch 0100/0300 | Cost: 0.4251\n",
      "Epoch: 015/020 | Batch 0200/0300 | Cost: 0.5481\n",
      "Epoch: 015/020 | Batch 0300/0300 | Cost: 0.3893\n",
      "epoch 15, skip_num 0, loss 0.3204, train acc 88.593%, test acc 86.890%\n",
      "Epoch: 016/020 | Batch 0100/0300 | Cost: 0.2803\n",
      "Epoch: 016/020 | Batch 0200/0300 | Cost: 0.2485\n",
      "Epoch: 016/020 | Batch 0300/0300 | Cost: 0.2991\n",
      "epoch 16, skip_num 0, loss 0.3159, train acc 88.777%, test acc 86.740%\n",
      "Epoch: 017/020 | Batch 0100/0300 | Cost: 0.1040\n",
      "Epoch: 017/020 | Batch 0200/0300 | Cost: 0.1796\n",
      "Epoch: 017/020 | Batch 0300/0300 | Cost: 0.2305\n",
      "epoch 17, skip_num 0, loss 0.3102, train acc 88.993%, test acc 86.870%\n",
      "Epoch: 018/020 | Batch 0100/0300 | Cost: 0.3241\n",
      "Epoch: 018/020 | Batch 0200/0300 | Cost: 0.4565\n",
      "Epoch: 018/020 | Batch 0300/0300 | Cost: 0.3938\n",
      "epoch 18, skip_num 0, loss 0.3054, train acc 89.125%, test acc 86.970%\n",
      "Epoch: 019/020 | Batch 0100/0300 | Cost: 0.4658\n",
      "Epoch: 019/020 | Batch 0200/0300 | Cost: 0.4305\n",
      "Epoch: 019/020 | Batch 0300/0300 | Cost: 0.3594\n",
      "epoch 19, skip_num 0, loss 0.3006, train acc 89.312%, test acc 87.130%\n",
      "Epoch: 020/020 | Batch 0100/0300 | Cost: 0.2204\n",
      "Epoch: 020/020 | Batch 0200/0300 | Cost: 0.3633\n",
      "Epoch: 020/020 | Batch 0300/0300 | Cost: 0.2903\n",
      "epoch 20, skip_num 0, loss 0.2976, train acc 89.442%, test acc 87.200%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"LASG  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "model_copy = MNISTNet_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Round_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach()\n",
    "                        grad_agg[p_id] += g\n",
    "                        grad_worker[w_id][p_id] = g\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread / (2 * lr * (NUM_WORKERS ** 2))\n",
    "\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach()\n",
    "                            grad_agg[p_id] += g\n",
    "                            grad_worker[w_id][p_id] = g\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append((iter_num* 4 - skip_iter)*32*407050)\n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append((iter_num* 4 - skip_iter)*32*407050)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"LASG-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"LASG-full-iter-fa.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse  lr:0.005--h:0.01--epoch:20--worker:4\n",
      "Number_parameter: 4\n",
      "Element_parameter: 407050\n",
      "Element_parameter_sparse: 4071\n",
      "Epoch: 001/020 | Batch 0100/0300 | Cost: 0.6343\n",
      "Epoch: 001/020 | Batch 0200/0300 | Cost: 0.5639\n",
      "Epoch: 001/020 | Batch 0300/0300 | Cost: 0.4029\n",
      "epoch 1, loss 0.7911, train acc 75.107%, test acc 80.580%\n",
      "Epoch: 002/020 | Batch 0100/0300 | Cost: 0.4404\n",
      "Epoch: 002/020 | Batch 0200/0300 | Cost: 0.6992\n",
      "Epoch: 002/020 | Batch 0300/0300 | Cost: 0.5706\n",
      "epoch 2, loss 0.5036, train acc 82.632%, test acc 82.270%\n",
      "Epoch: 003/020 | Batch 0100/0300 | Cost: 0.3371\n",
      "Epoch: 003/020 | Batch 0200/0300 | Cost: 0.3777\n",
      "Epoch: 003/020 | Batch 0300/0300 | Cost: 0.5601\n",
      "epoch 3, loss 0.4573, train acc 84.152%, test acc 83.480%\n",
      "Epoch: 004/020 | Batch 0100/0300 | Cost: 0.2723\n",
      "Epoch: 004/020 | Batch 0200/0300 | Cost: 0.5828\n",
      "Epoch: 004/020 | Batch 0300/0300 | Cost: 0.6271\n",
      "epoch 4, loss 0.4311, train acc 84.980%, test acc 84.130%\n",
      "Epoch: 005/020 | Batch 0100/0300 | Cost: 0.4935\n",
      "Epoch: 005/020 | Batch 0200/0300 | Cost: 0.5819\n",
      "Epoch: 005/020 | Batch 0300/0300 | Cost: 0.5648\n",
      "epoch 5, loss 0.4116, train acc 85.757%, test acc 84.380%\n",
      "Epoch: 006/020 | Batch 0100/0300 | Cost: 0.4524\n",
      "Epoch: 006/020 | Batch 0200/0300 | Cost: 0.3886\n",
      "Epoch: 006/020 | Batch 0300/0300 | Cost: 0.6547\n",
      "epoch 6, loss 0.3965, train acc 86.132%, test acc 84.860%\n",
      "Epoch: 007/020 | Batch 0100/0300 | Cost: 0.3673\n",
      "Epoch: 007/020 | Batch 0200/0300 | Cost: 0.2058\n",
      "Epoch: 007/020 | Batch 0300/0300 | Cost: 0.3981\n",
      "epoch 7, loss 0.3829, train acc 86.735%, test acc 85.240%\n",
      "Epoch: 008/020 | Batch 0100/0300 | Cost: 0.3534\n",
      "Epoch: 008/020 | Batch 0200/0300 | Cost: 0.3484\n",
      "Epoch: 008/020 | Batch 0300/0300 | Cost: 0.3907\n",
      "epoch 8, loss 0.3725, train acc 86.980%, test acc 85.320%\n",
      "Epoch: 009/020 | Batch 0100/0300 | Cost: 0.3363\n",
      "Epoch: 009/020 | Batch 0200/0300 | Cost: 0.3407\n",
      "Epoch: 009/020 | Batch 0300/0300 | Cost: 0.4879\n",
      "epoch 9, loss 0.3625, train acc 87.343%, test acc 85.310%\n",
      "Epoch: 010/020 | Batch 0100/0300 | Cost: 0.4513\n",
      "Epoch: 010/020 | Batch 0200/0300 | Cost: 0.5108\n",
      "Epoch: 010/020 | Batch 0300/0300 | Cost: 0.6250\n",
      "epoch 10, loss 0.3542, train acc 87.568%, test acc 85.840%\n",
      "Epoch: 011/020 | Batch 0100/0300 | Cost: 0.2685\n",
      "Epoch: 011/020 | Batch 0200/0300 | Cost: 0.4472\n",
      "****************************************************************************************************\n",
      "Iter_num: 3200 Test_acc 86.11999999999999 Comm_round: 12800\n",
      "****************************************************************************************************\n",
      "Epoch: 011/020 | Batch 0300/0300 | Cost: 0.2510\n",
      "epoch 11, loss 0.3468, train acc 87.797%, test acc 86.180%\n",
      "Epoch: 012/020 | Batch 0100/0300 | Cost: 0.2714\n",
      "Epoch: 012/020 | Batch 0200/0300 | Cost: 0.6972\n",
      "Epoch: 012/020 | Batch 0300/0300 | Cost: 0.2763\n",
      "epoch 12, loss 0.3390, train acc 88.078%, test acc 86.110%\n",
      "Epoch: 013/020 | Batch 0100/0300 | Cost: 0.4668\n",
      "Epoch: 013/020 | Batch 0200/0300 | Cost: 0.3828\n",
      "Epoch: 013/020 | Batch 0300/0300 | Cost: 0.3370\n",
      "epoch 13, loss 0.3335, train acc 88.225%, test acc 86.540%\n",
      "Epoch: 014/020 | Batch 0100/0300 | Cost: 0.2953\n",
      "Epoch: 014/020 | Batch 0200/0300 | Cost: 0.5251\n",
      "Epoch: 014/020 | Batch 0300/0300 | Cost: 0.4233\n",
      "epoch 14, loss 0.3268, train acc 88.497%, test acc 86.700%\n",
      "Epoch: 015/020 | Batch 0100/0300 | Cost: 0.1538\n",
      "Epoch: 015/020 | Batch 0200/0300 | Cost: 0.3392\n",
      "Epoch: 015/020 | Batch 0300/0300 | Cost: 0.3751\n",
      "epoch 15, loss 0.3210, train acc 88.697%, test acc 86.750%\n",
      "Epoch: 016/020 | Batch 0100/0300 | Cost: 0.3054\n",
      "Epoch: 016/020 | Batch 0200/0300 | Cost: 0.2235\n",
      "Epoch: 016/020 | Batch 0300/0300 | Cost: 0.2944\n",
      "epoch 16, loss 0.3151, train acc 88.905%, test acc 86.780%\n",
      "Epoch: 017/020 | Batch 0100/0300 | Cost: 0.2897\n",
      "Epoch: 017/020 | Batch 0200/0300 | Cost: 0.2965\n",
      "Epoch: 017/020 | Batch 0300/0300 | Cost: 0.3136\n",
      "epoch 17, loss 0.3105, train acc 89.027%, test acc 87.010%\n",
      "Epoch: 018/020 | Batch 0100/0300 | Cost: 0.3432\n",
      "Epoch: 018/020 | Batch 0200/0300 | Cost: 0.3573\n",
      "Epoch: 018/020 | Batch 0300/0300 | Cost: 0.2666\n",
      "epoch 18, loss 0.3054, train acc 89.197%, test acc 87.250%\n",
      "Epoch: 019/020 | Batch 0100/0300 | Cost: 0.2640\n",
      "Epoch: 019/020 | Batch 0200/0300 | Cost: 0.5109\n",
      "Epoch: 019/020 | Batch 0300/0300 | Cost: 0.3154\n",
      "epoch 19, loss 0.3006, train acc 89.385%, test acc 87.060%\n",
      "Epoch: 020/020 | Batch 0100/0300 | Cost: 0.1688\n",
      "Epoch: 020/020 | Batch 0200/0300 | Cost: 0.3253\n",
      "Epoch: 020/020 | Batch 0300/0300 | Cost: 0.2384\n",
      "epoch 20, loss 0.2959, train acc 89.515%, test acc 87.390%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparse  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)\n",
    "\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Loss_iter = []\n",
    "Round_epoch = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0.0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                    Tk_sparse = top_k_opt(g, h)\n",
    "                    grad_agg[p_id] += Tk_sparse\n",
    "                    error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append(iter_num* 4*4070*32 )\n",
    "          \n",
    "         \n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append(iter_num* 4*4070*32 )\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-full-iter-fa.csv\", encoding='gbk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dis-SGD  lr:0.1--epoch:10--worker:4\n",
      "Number_parameter: 10\n",
      "Element_parameter: 43508\n",
      "Epoch: 001/010 | Batch 0100/0500 | Cost: 2.5449\n",
      "Epoch: 001/010 | Batch 0200/0500 | Cost: 2.3472\n",
      "Epoch: 001/010 | Batch 0300/0500 | Cost: 2.2419\n",
      "Epoch: 001/010 | Batch 0400/0500 | Cost: 2.5265\n",
      "Epoch: 001/010 | Batch 0500/0500 | Cost: 2.3226\n",
      "epoch 1, loss 2.3977, train acc 9.860%, test acc 10.170%\n",
      "Epoch: 002/010 | Batch 0100/0500 | Cost: 2.5455\n",
      "Epoch: 002/010 | Batch 0200/0500 | Cost: 2.4242\n",
      "Epoch: 002/010 | Batch 0300/0500 | Cost: 2.2111\n",
      "Epoch: 002/010 | Batch 0400/0500 | Cost: 2.1968\n",
      "Epoch: 002/010 | Batch 0500/0500 | Cost: 2.3446\n",
      "epoch 2, loss 2.3527, train acc 10.505%, test acc 10.300%\n",
      "Epoch: 003/010 | Batch 0100/0500 | Cost: 2.3252\n",
      "Epoch: 003/010 | Batch 0200/0500 | Cost: 2.4217\n",
      "Epoch: 003/010 | Batch 0300/0500 | Cost: 2.5251\n",
      "Epoch: 003/010 | Batch 0400/0500 | Cost: 2.5184\n",
      "Epoch: 003/010 | Batch 0500/0500 | Cost: 2.4047\n",
      "epoch 3, loss 2.3956, train acc 10.220%, test acc 9.950%\n",
      "Epoch: 004/010 | Batch 0100/0500 | Cost: 2.5644\n",
      "Epoch: 004/010 | Batch 0200/0500 | Cost: 2.4362\n",
      "Epoch: 004/010 | Batch 0300/0500 | Cost: 2.4731\n",
      "Epoch: 004/010 | Batch 0400/0500 | Cost: 2.5421\n",
      "Epoch: 004/010 | Batch 0500/0500 | Cost: 2.2353\n",
      "epoch 4, loss 2.3973, train acc 9.928%, test acc 11.200%\n",
      "Epoch: 005/010 | Batch 0100/0500 | Cost: 2.2173\n",
      "Epoch: 005/010 | Batch 0200/0500 | Cost: 2.4557\n",
      "Epoch: 005/010 | Batch 0300/0500 | Cost: 2.3988\n",
      "Epoch: 005/010 | Batch 0400/0500 | Cost: 2.2668\n",
      "Epoch: 005/010 | Batch 0500/0500 | Cost: 2.3772\n",
      "epoch 5, loss 2.3448, train acc 10.520%, test acc 10.390%\n",
      "Epoch: 006/010 | Batch 0100/0500 | Cost: 2.2675\n",
      "Epoch: 006/010 | Batch 0200/0500 | Cost: 2.4220\n",
      "Epoch: 006/010 | Batch 0300/0500 | Cost: 2.2291\n",
      "Epoch: 006/010 | Batch 0400/0500 | Cost: 2.3206\n",
      "Epoch: 006/010 | Batch 0500/0500 | Cost: 2.2791\n",
      "epoch 6, loss 2.3759, train acc 9.760%, test acc 9.040%\n",
      "Epoch: 007/010 | Batch 0100/0500 | Cost: 2.4500\n",
      "Epoch: 007/010 | Batch 0200/0500 | Cost: 2.4930\n",
      "Epoch: 007/010 | Batch 0300/0500 | Cost: 2.3555\n",
      "Epoch: 007/010 | Batch 0400/0500 | Cost: 2.4669\n",
      "Epoch: 007/010 | Batch 0500/0500 | Cost: 2.3191\n",
      "epoch 7, loss 2.3386, train acc 10.208%, test acc 10.130%\n",
      "Epoch: 008/010 | Batch 0100/0500 | Cost: 2.4487\n",
      "Epoch: 008/010 | Batch 0200/0500 | Cost: 2.3815\n",
      "Epoch: 008/010 | Batch 0300/0500 | Cost: 2.2661\n",
      "Epoch: 008/010 | Batch 0400/0500 | Cost: 2.2371\n",
      "Epoch: 008/010 | Batch 0500/0500 | Cost: 2.5104\n",
      "epoch 8, loss 2.3512, train acc 10.083%, test acc 10.000%\n",
      "Epoch: 009/010 | Batch 0100/0500 | Cost: 2.3152\n",
      "Epoch: 009/010 | Batch 0200/0500 | Cost: 2.3635\n",
      "Epoch: 009/010 | Batch 0300/0500 | Cost: 2.3640\n",
      "Epoch: 009/010 | Batch 0400/0500 | Cost: 2.3811\n",
      "Epoch: 009/010 | Batch 0500/0500 | Cost: 2.3930\n",
      "epoch 9, loss 2.3545, train acc 10.098%, test acc 9.800%\n",
      "Epoch: 010/010 | Batch 0100/0500 | Cost: 2.4675\n",
      "Epoch: 010/010 | Batch 0200/0500 | Cost: 2.4016\n",
      "Epoch: 010/010 | Batch 0300/0500 | Cost: 2.2644\n",
      "Epoch: 010/010 | Batch 0400/0500 | Cost: 2.3182\n",
      "Epoch: 010/010 | Batch 0500/0500 | Cost: 2.3584\n",
      "epoch 10, loss 2.3710, train acc 10.152%, test acc 10.000%\n",
      "Finished.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (4) does not match length of index (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m list_write\u001b[38;5;241m.\u001b[39mappend(Round_epoch)\n\u001b[1;32m    105\u001b[0m name \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain-acc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 106\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_write\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    107\u001b[0m test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./result/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSGD-full-fa.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m list_write_iter \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py:754\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    745\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    746\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    747\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m         dtype,\n\u001b[1;32m    753\u001b[0m     )\n\u001b[0;32m--> 754\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    763\u001b[0m         data,\n\u001b[1;32m    764\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    768\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    769\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/internals/construction.py:123\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/internals/construction.py:620\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    615\u001b[0m             val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m    617\u001b[0m         val \u001b[38;5;241m=\u001b[39m sanitize_array(\n\u001b[1;32m    618\u001b[0m             val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, raise_cast_failure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[0;32m--> 620\u001b[0m         \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m homogenized\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (4) does not match length of index (3)"
     ]
    }
   ],
   "source": [
    "print(\"Dis-SGD  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = LeNet()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "Round_epoch = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    g = lr * p.grad.data.clone().detach()\n",
    "                    g_clip = clip_grad_norm_(g, max_norm=20, norm_type=2)\n",
    "                    noise = np.random.normal(mean, var**1, g_clip.shape)\n",
    "                    g_clip_noise = g_clip + noise\n",
    "                    grad_agg[p_id] += g_clip_noise\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append(iter_num* 4 *32*407050)\n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append(iter_num* 4 *32*407050)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)\n",
    "#                 print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SGD-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SGD-full-iter-fa.csv\", encoding='gbk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original communication number: 1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[1;32m     85\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomm_round_dis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest-acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSGD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# plt.plot(comm_round_sparse, sparse_data['test-acc'].values.tolist(), 'b', label='Sparse', linewidth=2.5, linestyle='-')\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# plt.plot(comm_round_LASG, LASG_data['test-acc'].values.tolist(), 'g', label='LASG', linewidth=2.5)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# plt.plot(comm_round_SASG, SASG_data['test-acc'].values.tolist(), 'r', label='SASG', linewidth=2.5)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# plt.plot(comm_round_TASGS, TASGS_data['test-acc'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/pyplot.py:2748\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1668\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1668\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (50,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record once every 100 iterations, and 10 workers need to communicate in each iteration\n",
    "original_num = 1000\n",
    "k=1\n",
    "print(\"original communication number:\", original_num)\n",
    "\n",
    "TASGS_data = pd.read_csv(\"./result/\"+\"TASGS-full-iter-fa.csv\")\n",
    "SASG_data = pd.read_csv(\"./result/\"+\"SASG-full-iter-fa.csv\")\n",
    "LASG_data = pd.read_csv(\"./result/\"+\"LASG-full-iter-fa.csv\")\n",
    "sparse_data = pd.read_csv(\"./result/\"+\"Sparse-full-iter-fa.csv\")\n",
    "sgd_data = pd.read_csv(\"./result/\"+\"SGD-full-iter-fa.csv\")\n",
    "\n",
    "TASGS_skip = TASGS_data['Skip'].values.tolist()\n",
    "SASG_skip = SASG_data['Skip'].values.tolist()\n",
    "LASG_skip = LASG_data['Skip'].values.tolist()\n",
    "\n",
    "comm_round_TASGS = []\n",
    "comm_round_SASG = []\n",
    "comm_round_LASG = []\n",
    "comm_round_sparse = []\n",
    "comm_round_dis = []\n",
    "\n",
    "comm_bit_TASGS = []\n",
    "comm_bit_SASG = []\n",
    "comm_bit_LASG = []\n",
    "comm_bit_sparse = []\n",
    "comm_bit_dis = []\n",
    "\n",
    "Eopch_TASGS = []\n",
    "Eopch_SASG = []\n",
    "Eopch_LASG = []\n",
    "Eopch_sparse = []\n",
    "Eopch_dis = []\n",
    "\n",
    "\n",
    "\n",
    "comm_num_TASGS, comm_num_SASG, comm_num_LASG, comm_num_sparse, comm_num_dis = 0, 0, 0, 0 ,0\n",
    "Eopch_n_TASGS,Eopch_n_SASG,Eopch_n_LASG,Eopch_n_sparse,Eopch_n_dis=0,0,0,0,0\n",
    "for i in range(len(TASGS_skip)):\n",
    "    \n",
    "    comm_num_TASGS += original_num\n",
    "    a= comm_num_TASGS - TASGS_skip[i]\n",
    "#     print(\"总轮次\",a)\n",
    "    comm_round_TASGS.append(a)\n",
    "    \n",
    "    \n",
    "for i in range(len(SASG_skip)):\n",
    "    comm_num_SASG += original_num\n",
    "    b=comm_num_SASG - SASG_skip[i]\n",
    "\n",
    "    comm_round_SASG.append(b)\n",
    "\n",
    "for i in range(len(LASG_skip)):\n",
    "    comm_num_LASG += original_num\n",
    "    comm_num_sparse += original_num\n",
    "    comm_num_dis += original_num\n",
    "    \n",
    "    c=comm_num_LASG - LASG_skip[i]\n",
    "\n",
    "\n",
    "    comm_round_LASG.append(c)\n",
    "    comm_round_sparse.append(comm_num_sparse)\n",
    "comm_round_dis.append(comm_num_dis)\n",
    "\n",
    "\n",
    "for k in range(20):\n",
    "    Eopch_n_TASGS += k\n",
    "    Eopch_n_SASG += k\n",
    "    Eopch_n_LASG += k\n",
    "    Eopch_n_sparse += k\n",
    "    Eopch_n_dis += k\n",
    "    \n",
    "    Eopch_TASGS.append(Eopch_n_TASGS)\n",
    "    Eopch_SASG.append(Eopch_n_SASG)\n",
    "    Eopch_LASG.append(Eopch_n_LASG)\n",
    "    Eopch_sparse.append(Eopch_n_sparse)\n",
    "    Eopch_dis.append(Eopch_n_dis)\n",
    "    \n",
    "font1 = {'weight': 'normal', 'size': 17}\n",
    "font2 = {'weight': 'normal', 'size': 20}\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_dis, sgd_data['test-acc'].values.tolist(), 'black', label='SGD', linewidth=2.5, linestyle='-')\n",
    "# plt.plot(comm_round_sparse, sparse_data['test-acc'].values.tolist(), 'b', label='Sparse', linewidth=2.5, linestyle='-')\n",
    "# plt.plot(comm_round_LASG, LASG_data['test-acc'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "# plt.plot(comm_round_SASG, SASG_data['test-acc'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "# plt.plot(comm_round_TASGS, TASGS_data['test-acc'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(40000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.ylim(75, 90)\n",
    "ymajorLocator = MultipleLocator(2)  # major\n",
    "ax.yaxis.set_major_locator(ymajorLocator)\n",
    "yminorLocator = MultipleLocator(1)  # minor\n",
    "ax.yaxis.set_minor_locator(yminorLocator)\n",
    "plt.ylabel('Test Accuracy', font2)\n",
    "\n",
    "\n",
    "\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.13, right=0.95, bottom=0.15, top=0.95)\n",
    "\n",
    "plt.savefig(\"./result/\"+\"test_full.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_dis, sgd_data['Loss'].values.tolist(), 'black', label='SGD', linewidth=2.5, linestyle='-')\n",
    "# plt.plot(comm_round_sparse, sparse_data['Loss'].values.tolist(), 'b', label='Sparse', linewidth=2.5, linestyle='-.')\n",
    "# plt.plot(comm_round_LASG, LASG_data['Loss'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "# plt.plot(comm_round_SASG, SASG_data['Loss'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "# plt.plot(comm_round_TASGS, TASGS_data['Loss'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(40000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "# ax.ticklabel_format(axis='y', style='plain', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss', font2)\n",
    "# plt.ylim(0, 2)\n",
    "# ax.set_yticks((0.1, 1,)\n",
    "# ax.set_yticklabels([ '$10^{0}$', '$ 10^{-1}$'], fontsize=17)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.16, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"loss_full.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original communication number: 1000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 98\u001b[0m\n\u001b[1;32m     94\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(comm_round_TASGS_noise1, TASGS_noise1_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGSASG_noise_0.05\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     97\u001b[0m xmajorLocator \u001b[38;5;241m=\u001b[39m MultipleLocator(\u001b[38;5;241m80000\u001b[39m)  \u001b[38;5;66;03m# m\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mset_major_locator(xmajorLocator)\n\u001b[1;32m     99\u001b[0m xminorLocator \u001b[38;5;241m=\u001b[39m MultipleLocator(\u001b[38;5;241m20000\u001b[39m)  \u001b[38;5;66;03m# minor\u001b[39;00m\n\u001b[1;32m    100\u001b[0m ax\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mset_minor_locator(xminorLocator)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ax' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record once every 100 iterations, and 10 workers need to communicate in each iteration\n",
    "original_num = 1000\n",
    "k=1\n",
    "print(\"original communication number:\", original_num)\n",
    "\n",
    "TASGS_data = pd.read_csv(\"./result/\"+\"TASGS-full-iter-fa.csv\")\n",
    "TASGS_noise_data = pd.read_csv(\"./result/\"+\"TASGS-full-iter-fa-nosie.csv\")\n",
    "TASGS_noise1_data = pd.read_csv(\"./result/\"+\"TASGS-full-iter-fa-nosie1.csv\")\n",
    "\n",
    "\n",
    "TASGS_skip = TASGS_data['Skip'].values.tolist()\n",
    "TASGS_noise_skip = TASGS_noise_data['Skip'].values.tolist()\n",
    "TASGS_noise1_skip = TASGS_noise1_data['Skip'].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "comm_round_TASGS = []\n",
    "comm_round_TASGS_noise = []\n",
    "comm_round_TASGS_noise1 = []\n",
    "\n",
    "\n",
    "comm_bit_TASGS = []\n",
    "comm_bit_TASGS_noise = []\n",
    "comm_bit_TASGS_noise1 = []\n",
    "\n",
    "\n",
    "Eopch_TASGS = []\n",
    "Eopch_TASGS_noise = []\n",
    "Eopch_TASGS_noise1 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "comm_num_TASGS, comm_num_TASGS_noise,comm_num_TASGS_noise1 = 0, 0, 0\n",
    "Eopch_n_TASGS,Eopch_n_TASGS_noise,Eopch_n_TASGS_noise1=0,0,0\n",
    "for i in range(len(TASGS_skip)):\n",
    "    \n",
    "    comm_num_TASGS += original_num\n",
    "    a= comm_num_TASGS - TASGS_skip[i]\n",
    "#     print(\"总轮次\",a)\n",
    "    comm_round_TASGS.append(a)\n",
    "    \n",
    "    \n",
    "for i in range(len(TASGS_noise_skip)):\n",
    "    comm_num_TASGS_noise += original_num\n",
    "    b=comm_num_TASGS_noise - TASGS_noise_skip[i]\n",
    "\n",
    "    comm_round_TASGS_noise.append(b)\n",
    "    \n",
    "    \n",
    "for i in range(len(TASGS_noise1_skip)):\n",
    "    comm_num_TASGS_noise1 += original_num\n",
    "    c=comm_num_TASGS_noise1 - TASGS_noise1_skip[i]\n",
    "\n",
    "    comm_round_TASGS_noise1.append(c)\n",
    "\n",
    "# for i in range(len(LASG_skip)):\n",
    "#     comm_num_LASG += original_num\n",
    "#     comm_num_sparse += original_num\n",
    "#     comm_num_dis += original_num\n",
    "    \n",
    "#     c=comm_num_LASG - LASG_skip[i]\n",
    "\n",
    "\n",
    "#     comm_round_LASG.append(c)\n",
    "#     comm_round_sparse.append(comm_num_sparse)\n",
    "#     comm_round_dis.append(comm_num_dis)\n",
    "\n",
    "\n",
    "# for k in range(20):\n",
    "#     Eopch_n_TASGS += k\n",
    "#     Eopch_n_SASG += k\n",
    "#     Eopch_n_LASG += k\n",
    "#     Eopch_n_sparse += k\n",
    "#     Eopch_n_dis += k\n",
    "    \n",
    "#     Eopch_TASGS.append(Eopch_n_TASGS)\n",
    "#     Eopch_SASG.append(Eopch_n_SASG)\n",
    "#     Eopch_LASG.append(Eopch_n_LASG)\n",
    "#     Eopch_sparse.append(Eopch_n_sparse)\n",
    "#     Eopch_dis.append(Eopch_n_dis)\n",
    "    \n",
    "font1 = {'weight': 'normal', 'size': 17}\n",
    "font2 = {'weight': 'normal', 'size': 20}\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_TASGS, TASGS_data['test-acc'].values.tolist(), 'c', label='GSASG', linewidth=2.5, linestyle='-')\n",
    "plt.plot(comm_round_TASGS_noise, TASGS_noise_data['test-acc'].values.tolist(), 'r', label='GSASG_noise', linewidth=2.5, linestyle='-')\n",
    "plt.plot(comm_round_TASGS_noise1, TASGS_noise1_data['test-acc'].values.tolist(), 'b', label='GSASG_noise_0.05', linewidth=2.5, linestyle='-')\n",
    "\n",
    "\n",
    "xmajorLocator = MultipleLocator(80000)  # m\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.ylim(75, 90)\n",
    "ymajorLocator = MultipleLocator(2)  # major\n",
    "ax.yaxis.set_major_locator(ymajorLocator)\n",
    "yminorLocator = MultipleLocator(1)  # minor\n",
    "ax.yaxis.set_minor_locator(yminorLocator)\n",
    "plt.ylabel('Test Accuracy', font2)\n",
    "\n",
    "\n",
    "\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.13, right=0.95, bottom=0.15, top=0.95)\n",
    "\n",
    "plt.savefig(\"./result/\"+\"test_full.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "\n",
    "plt.plot(comm_round_TASGS, TASGS_data['Loss'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\n",
    "plt.plot(comm_round_TASGS_noise, TASGS_noise_data['Loss'].values.tolist(), 'r', label='GSASG_noise', linewidth=2.5)\n",
    "plt.plot(comm_round_TASGS_noise1, TASGS_noise1_data['Loss'].values.tolist(), 'b', label='GSASG_noise_0.05', linewidth=2.5)\n",
    "\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(40000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "# ax.ticklabel_format(axis='y', style='plain', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss', font2)\n",
    "# plt.ylim(0, 2)\n",
    "# ax.set_yticks((0.1, 1,)\n",
    "# ax.set_yticklabels([ '$10^{0}$', '$ 10^{-1}$'], fontsize=17)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.16, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"loss_full.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
