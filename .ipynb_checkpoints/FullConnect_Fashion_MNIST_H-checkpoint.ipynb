{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Result for **FullConnect+MNIST**\n",
    "### This notebook is for seed $42$. The results in the paper are the average of $5$ times with seeds in $[1, 19, 31, 42, 80]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "# import skimage\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MNIST Fully Connected Net ----------------------------------------------------\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MNISTNet_copy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet_copy, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_loaders(dataset_name, n_workers, batch_size):\n",
    "    train_data, test_data = load_data(dataset_name)\n",
    "    train_loader_workers = dict()\n",
    "    n = len(train_data)\n",
    "    \n",
    "    # preparing iterators for workers\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    a = np.int64(np.floor(n / n_workers))\n",
    "    top_ind = a * n_workers\n",
    "    seq = range(a, top_ind, a)\n",
    "    split = np.split(indices[:top_ind], seq)\n",
    "    b = 0\n",
    "    for ind in split:\n",
    "        train_loader_workers[b] = DataLoader(Subset(train_data, ind), batch_size=batch_size, shuffle=True)\n",
    "        b = b + 1\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_workers, test_loader\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'mnist':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        train_data = datasets.MNIST(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "        test_data = datasets.MNIST(root='./data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "    elif dataset_name == 'fashionmnist':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        train_data = datasets.FashionMNIST(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "        test_data = datasets.FashionMNIST(root='./data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "\n",
    "\n",
    "    elif dataset_name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        train_data = datasets.CIFAR10(root='./data', train=True,\n",
    "                                      download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR10(root='./data', train=False,\n",
    "                                     download=True, transform=transform_test)\n",
    "\n",
    "    elif dataset_name == 'cifar100':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        train_data = datasets.CIFAR100(root='./data', train=True,\n",
    "                                       download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR100(root='./data', train=False,\n",
    "                                      download=True, transform=transform_test)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(dataset_name + ' is not known.')\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parameter and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "lr = 0.005\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 20\n",
    "NUM_WORKERS = 4\n",
    "D = 10  # compute thread\n",
    "h = 0.01  # sparse level 99%\n",
    "h1= 0.005 # gsparse level\n",
    "\n",
    "\n",
    "mean = 0\n",
    "var = 0.05\n",
    "#var表示全局敏感度*delta，全局敏感=1，delta=1.4，c=1\n",
    "# RGB\n",
    "# noise = sigma * np.random.randn(256, 256, 3)#高斯噪声\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \n",
    "def evaluate_accuracy(model, data_iter, device):\n",
    "    acc_sum, num_epo = 0.0, 0\n",
    "    for image, label in data_iter:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        acc_sum += (model(image).argmax(dim=1) == label).float().sum().item()\n",
    "        num_epo += label.shape[0]\n",
    "    return acc_sum / num_epo * 100\n",
    "\n",
    "\n",
    "# 定义Top_k sparse\n",
    "def prep_grad(x):\n",
    "    x_flat = torch.unsqueeze(x, 0).flatten()\n",
    "    dim = x.shape#dim代表维度，相当于有多少个数\n",
    "    d = x_flat.shape[0]\n",
    "    return x_flat, dim, d\n",
    "\n",
    "\n",
    "def top_k_opt(x, h):\n",
    "    \"\"\"\n",
    "    :param x: vector to sparsify\n",
    "    :param h: density\n",
    "    :return: compressed vector\n",
    "    \"\"\"\n",
    "    x, dim, d = prep_grad(x)\n",
    "    # number of coordinates kept保留的坐标数，保留top多少 r就是这个k的数量\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    # positions of top_k coordinates topk的坐标位置，保留了值和索引\n",
    "    _, ind = torch.topk(torch.abs(x), r)\n",
    "    mask = torch.zeros_like(x)#生成与x相同的全0张量\n",
    "    mask[ind] = 1#把索引那一行全置为1\n",
    "    t = mask * x\n",
    "    #上面是top-k\n",
    "    t = t.reshape(dim)\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "def compt(old, new):\n",
    "    result = 0\n",
    "    for i in range(len(old)):\n",
    "        result += ((old[i].view(-1) - new[i].view(-1)) ** 2).sum().item()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GTOP-K-SASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGSASG  lr:0.005--h:0.01--epoch:20--worker:4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_six'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# prepare data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m train_loader_workers, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m NUM_PARAS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber_parameter:\u001b[39m\u001b[38;5;124m\"\u001b[39m, NUM_PARAS)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mcreate_loaders\u001b[0;34m(dataset_name, n_workers, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_loaders\u001b[39m(dataset_name, n_workers, batch_size):\n\u001b[0;32m----> 2\u001b[0m     train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     train_loader_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m      4\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data)\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     25\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     26\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m,), (\u001b[38;5;241m0.3081\u001b[39m,))\n\u001b[1;32m     27\u001b[0m     ])\n\u001b[0;32m---> 28\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m                                download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfashionmnist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/mnist.py:91\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     85\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     download: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_exist():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/vision.py:39\u001b[0m, in \u001b[0;36mVisionDataset.__init__\u001b[0;34m(self, root, transforms, transform, target_transform)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     33\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     target_transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     _log_api_usage_once(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_six\u001b[49m\u001b[38;5;241m.\u001b[39mstring_classes):\n\u001b[1;32m     40\u001b[0m         root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m root\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_six'"
     ]
    }
   ],
   "source": [
    "print(\"HGSASG  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "model_copy = MNISTNet_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 计算损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "total_params_sparse_gtop = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)#\n",
    "    r = int(np.maximum(1, np.floor(d * h)))#h是代表稀疏化程度百分之99，floor向下取整\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)#稀疏化之后参与计算的参数也就是参数数量\n",
    "\n",
    "for p in model.parameters():\n",
    "    u = top_k_opt(p, h)\n",
    "    x1, dim1, d1 = prep_grad(u)#\n",
    "    s = int(np.maximum(1, np.floor(d1 * h)))#h是代表稀疏化程度百分之99，floor向下取整\n",
    "    total_params_sparse_gtop += s\n",
    "print(\"Element_parameter_sparse_gtop:\", total_params_sparse_gtop)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行\n",
    "grad_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行  新加的！！！！\n",
    "grad_igtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行  新加的！！！！\n",
    "\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]# 新加的！！！！\n",
    "error_igtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]# 新加的！！！！\n",
    "\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []#跳过的轮次\n",
    "Loss_epoch = []#损失的轮次\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "Round_epoch = []\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "L=0\n",
    "H=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "   \n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0.0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "        grad_igtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "        \n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "      \n",
    "            for w_id in range(NUM_WORKERS):\n",
    "#                 print(NUM_WORKERS)\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():#每个work的本地训练循环\n",
    "                        #本地Top-k稀疏化\n",
    "                        g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)\n",
    "                        #本地聚合操作\n",
    "                        grad_agg[p_id] += Tk_sparse\n",
    "                        #本地错误累计操作\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        \n",
    "                        #本地进行局部Top-k操作\n",
    "                        IGTk_sparse = top_k_opt(grad_worker[w_id][p_id]+error_igtworker[w_id][p_id] , h)\n",
    "                        #上传服务器聚合\n",
    "                        grad_igtagg[p_id] += IGTk_sparse#11111111\n",
    "                        #累计igt误差\n",
    "                        error_igtworker[w_id][p_id] = Tk_sparse-IGTk_sparse\n",
    "                        grad_igtworker[w_id][p_id] = IGTk_sparse\n",
    "                        \n",
    "                        #本地上传的梯度\n",
    "                        #服务器端操作\n",
    "                        GTk_sparse = top_k_opt(grad_igtworker[w_id][p_id]+error_gtworker[w_id][p_id], h)#111111\n",
    "                        grad_gtagg[p_id] += GTk_sparse#11111111\n",
    "                        error_gtworker[w_id][p_id] = IGTk_sparse - GTk_sparse#1111111\n",
    "                        grad_gtworker[w_id][p_id] = GTk_sparse#1111111\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "                \n",
    "        else:\n",
    "      \n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (2 * lr * (NUM_WORKERS ** 2))\n",
    "\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    " \n",
    "                \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "                    \n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "#                             g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "#                             Tk_sparse = top_k_opt(g, h)#本地top-k稀疏化\n",
    "#                             grad_agg[p_id] += Tk_sparse\n",
    "#                             error_worker[w_id][p_id] = g - Tk_sparse\n",
    "#                             grad_worker[w_id][p_id] = Tk_sparse\n",
    "#                             GTk_sparse = top_k_opt(grad_worker[w_id][p_id], h1)#1111112\n",
    "#                             grad_gtagg[p_id] += GTk_sparse#111111112\n",
    "#                             error_gtworker[w_id][p_id] = error_worker[w_id][p_id] + (Tk_sparse - GTk_sparse)#11111112\n",
    "#                             grad_gtworker[w_id][p_id] = GTk_sparse#11111112\n",
    "\n",
    "                                       #本地Top-k稀疏化\n",
    "                            g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)\n",
    "                            #本地聚合操作\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            #本地错误累计操作\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        \n",
    "                            #本地进行局部Top-k操作\n",
    "                            IGTk_sparse = top_k_opt(grad_worker[w_id][p_id]+ error_igtworker[w_id][p_id], h)\n",
    "                            #上传服务器聚合\n",
    "                            grad_igtagg[p_id] += IGTk_sparse#11111111\n",
    "                            #累计igt误差1\n",
    "                            error_igtworker[w_id][p_id] = Tk_sparse-IGTk_sparse\n",
    "                            grad_igtworker[w_id][p_id] = IGTk_sparse\n",
    "                        \n",
    "                            #本地上传的梯度\n",
    "                            #服务器端操作\n",
    "                            GTk_sparse = top_k_opt(grad_igtworker[w_id][p_id]+error_gtworker[w_id][p_id], h)#111111\n",
    "                            grad_gtagg[p_id] += GTk_sparse#11111111\n",
    "                            error_gtworker[w_id][p_id] = IGTk_sparse - GTk_sparse#1111111\n",
    "                            grad_gtworker[w_id][p_id] = GTk_sparse#1111111\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]#111111\n",
    "                            grad_igtagg[p_id] += grad_igtworker[w_id][p_id]#11111111\n",
    "                            grad_gtagg[p_id] += grad_gtworker[w_id][p_id]#1111111\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)#111111\n",
    "                p.data.add_(grad_igtagg[p_id], alpha=-1)#111111               \n",
    "                p.data.add_(grad_gtagg[p_id], alpha=-1)#111111\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "                \n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "  \n",
    "                     \n",
    "#                      Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "                   \n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "# #                 Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "# save_csv\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-full-m-H.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-full-iter-m-H.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"GSASG  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "model_copy = MNISTNet_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 计算损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'mnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "total_params_sparse_gtop = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)#\n",
    "    r = int(np.maximum(1, np.floor(d * h)))#h是代表稀疏化程度百分之99，floor向下取整\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)#稀疏化之后参与计算的参数也就是参数数量\n",
    "\n",
    "for p in model.parameters():\n",
    "    u = top_k_opt(p, h)\n",
    "    x1, dim1, d1 = prep_grad(u)#\n",
    "    s = int(np.maximum(1, np.floor(d1 * h)))#h是代表稀疏化程度百分之99，floor向下取整\n",
    "    total_params_sparse_gtop += s\n",
    "print(\"Element_parameter_sparse_gtop:\", total_params_sparse_gtop)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行\n",
    "grad_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行  新加的！！！！\n",
    "grad_igtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]#col是列，row是行  新加的！！！！\n",
    "\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_gtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]# 新加的！！！！\n",
    "error_igtworker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]# 新加的！！！！\n",
    "\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []#跳过的轮次\n",
    "Loss_epoch = []#损失的轮次\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "Round_epoch = []\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "L=0\n",
    "H=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "   \n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "        grad_igtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "        \n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "      \n",
    "            for w_id in range(NUM_WORKERS):\n",
    "#                 print(NUM_WORKERS)\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():#每个work的本地训练循环\n",
    "                        #本地Top-k稀疏化\n",
    "                        g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)#本地top-k稀疏化\n",
    "                        grad_agg[p_id] += Tk_sparse\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        GTk_sparse = top_k_opt(grad_worker[w_id][p_id]+error_gtworker[w_id][p_id], h)#111111\n",
    "                        grad_gtagg[p_id] += GTk_sparse#11111111\n",
    "                        error_gtworker[w_id][p_id] = Tk_sparse - GTk_sparse#1111111\n",
    "                        grad_gtworker[w_id][p_id] = GTk_sparse#1111111\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "                \n",
    "        else:\n",
    "      \n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (2 * lr * (NUM_WORKERS ** 2))\n",
    "\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    " \n",
    "                \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    grad_gtagg = [0 for col in range(NUM_PARAS)]#新加的！！！！！\n",
    "                    \n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach() + error_gtworker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)#本地top-k稀疏化\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                            GTk_sparse = top_k_opt(grad_worker[w_id][p_id]+error_gtworker[w_id][p_id], h)#1111112\n",
    "                            grad_gtagg[p_id] += GTk_sparse#111111112\n",
    "                            error_gtworker[w_id][p_id] = Tk_sparse - GTk_sparse#11111112\n",
    "                            grad_gtworker[w_id][p_id] = GTk_sparse#11111112\n",
    "\n",
    "\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]#111111\n",
    "                            grad_igtagg[p_id] += grad_igtworker[w_id][p_id]#11111111\n",
    "                            grad_gtagg[p_id] += grad_gtworker[w_id][p_id]#1111111\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)#111111\n",
    "                p.data.add_(grad_igtagg[p_id], alpha=-1)#111111               \n",
    "                p.data.add_(grad_gtagg[p_id], alpha=-1)#111111\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "                \n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "  \n",
    "                     \n",
    "#                      Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "                   \n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "# #                 Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "# save_csv\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-full-m.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"TASGS-full-iter-m.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"SASG  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "model_copy = MNISTNet_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Round_epoch = []\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)\n",
    "                        grad_agg[p_id] += Tk_sparse\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (2 * lr * (NUM_WORKERS ** 2))\n",
    "\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "\n",
    "                \n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "                \n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "\n",
    "\n",
    "# #                 flag_acc = True\n",
    "# #                 print(\"*\" * 100)\n",
    "# #                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "# #                 print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append((iter_num* 4 - skip_iter)*32*4070)\n",
    "\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append(iter_num* 4 - skip_iter)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "#             print(Round_epoch)\n",
    "                \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "comm_iter = iter_num * 4 - skip_iter\n",
    "# save_csv\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SASG-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SASG-full-iter-fa.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# LASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"LASG  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "model_copy = MNISTNet_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Round_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach()\n",
    "                        grad_agg[p_id] += g\n",
    "                        grad_worker[w_id][p_id] = g\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread / (2 * lr * (NUM_WORKERS ** 2))\n",
    "\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach()\n",
    "                            grad_agg[p_id] += g\n",
    "                            grad_worker[w_id][p_id] = g\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append((iter_num* 4 - skip_iter)*32*407050)\n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append((iter_num* 4 - skip_iter)*32*407050)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 4 - skip_iter)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"LASG-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"LASG-full-iter-fa.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sparse  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)\n",
    "\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Loss_iter = []\n",
    "Round_epoch = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0.0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                    Tk_sparse = top_k_opt(g, h)\n",
    "                    grad_agg[p_id] += Tk_sparse\n",
    "                    error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append(iter_num* 4*4070*32 )\n",
    "          \n",
    "         \n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append(iter_num* 4*4070*32 )\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)  # 10 workers\n",
    "#                 print(\"*\" * 100)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-full-iter-fa.csv\", encoding='gbk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dis-SGD  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = MNISTNet()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'fashionmnist'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "Loss_epoch = []\n",
    "train_Acc_epoch = []\n",
    "test_Acc_epoch = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "Round_epoch = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    grad_agg[p_id] += lr * p.grad.data.clone().detach()\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 86.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "#             if test_acc_it < 97.0 and not flag_acc:\n",
    "#                 Round_epoch.append(iter_num* 4 *32*407050)\n",
    "\n",
    "    \n",
    "                \n",
    "#             if test_acc_it > 97.0 and not flag_acc:\n",
    "#                 flag_acc = True\n",
    "#                 Round_epoch.append(iter_num* 4 *32*407050)\n",
    "#                 print(\"*\" * 100)\n",
    "#                 print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 4)\n",
    "#                 print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_Acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_Acc_epoch.append(test_acc)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_Acc_epoch)\n",
    "list_write.append(test_Acc_epoch)\n",
    "list_write.append(Round_epoch)\n",
    "\n",
    "name = ['Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SGD-full-fa.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "list_write_iter.append(Round_epoch)\n",
    "\n",
    "name_iter = ['Loss', 'train-acc', 'test-acc','commd_round']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SGD-full-iter-fa.csv\", encoding='gbk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record once every 100 iterations, and 10 workers need to communicate in each iteration\n",
    "original_num = 1000\n",
    "k=1\n",
    "print(\"original communication number:\", original_num)\n",
    "\n",
    "TASGS_data = pd.read_csv(\"./result/\"+\"TASGS-full-iter-fa.csv\")\n",
    "HTASGS_data = pd.read_csv(\"./result/\"+\"TASGS-full-iter-fa-H.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TASGS_skip = TASGS_data['Skip'].values.tolist()\n",
    "HTASGS_skip = HTASGS_data['Skip'].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "comm_round_TASGS = []\n",
    "comm_round_HTASGS = []\n",
    "\n",
    "\n",
    "\n",
    "comm_bit_TASGS = []\n",
    "comm_bit_HTASGS = []\n",
    "\n",
    "\n",
    "Eopch_TASGS = []\n",
    "Eopch_HTASGS = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "comm_num_TASGS,comm_num_HTASGS = 0, 0\n",
    "Eopch_n_TASGS,Eopch_n_HTASGS=0,0\n",
    "for i in range(len(TASGS_skip)):\n",
    "    \n",
    "    comm_num_TASGS += original_num\n",
    "    a= comm_num_TASGS - TASGS_skip[i]\n",
    "#     print(\"总轮次\",a)\n",
    "    comm_round_TASGS.append(a)\n",
    "\n",
    "for i in range(len(HTASGS_skip)):\n",
    "    \n",
    "    comm_num_HTASGS += original_num\n",
    "    b= comm_num_HTASGS - HTASGS_skip[i]\n",
    "#     print(\"总轮次\",a)\n",
    "    comm_round_HTASGS.append(b)\n",
    "    \n",
    "\n",
    "    \n",
    "font1 = {'weight': 'normal', 'size': 17}\n",
    "font2 = {'weight': 'normal', 'size': 20}\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_TASGS, TASGS_data['test-acc'].values.tolist(), 'r', label='GSASG', linewidth=2.5, linestyle='-')\n",
    "plt.plot(comm_round_HTASGS, HTASGS_data['test-acc'].values.tolist(), 'c', label='HGSASG', linewidth=2.5, linestyle='-')\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(40000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.ylim(75, 90)\n",
    "ymajorLocator = MultipleLocator(2)  # major\n",
    "ax.yaxis.set_major_locator(ymajorLocator)\n",
    "yminorLocator = MultipleLocator(1)  # minor\n",
    "ax.yaxis.set_minor_locator(yminorLocator)\n",
    "plt.ylabel('Test Accuracy', font2)\n",
    "\n",
    "\n",
    "\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.13, right=0.95, bottom=0.15, top=0.95)\n",
    "\n",
    "plt.savefig(\"./result/\"+\"test_full.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "\n",
    "plt.plot(comm_round_TASGS, TASGS_data['Loss'].values.tolist(), 'r', label='GSASG', linewidth=2.5)\n",
    "plt.plot(comm_round_HTASGS, HTASGS_data['Loss'].values.tolist(), 'c', label='HGSASG', linewidth=2.5)\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(40000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "# ax.ticklabel_format(axis='y', style='plain', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss', font2)\n",
    "# plt.ylim(0, 2)\n",
    "# ax.set_yticks((0.1, 1,)\n",
    "# ax.set_yticklabels([ '$10^{0}$', '$ 10^{-1}$'], fontsize=17)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.16, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"loss_full.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Record once every 100 iterations, and 10 workers need to communicate in each iteration\n",
    "original_num = 1000\n",
    "k=1\n",
    "print(\"original communication number:\", original_num)\n",
    "\n",
    "TASGS_data = pd.read_csv(\"./result/\"+\"TASGS-full-iter-fa-H.csv\")\n",
    "SASG_data = pd.read_csv(\"./result/\"+\"SASG-full-iter-fa.csv\")\n",
    "LASG_data = pd.read_csv(\"./result/\"+\"LASG-full-iter-fa.csv\")\n",
    "sparse_data = pd.read_csv(\"./result/\"+\"Sparse-full-iter-fa.csv\")\n",
    "sgd_data = pd.read_csv(\"./result/\"+\"SGD-full-iter-fa.csv\")\n",
    "\n",
    "TASGS_skip = TASGS_data['Skip'].values.tolist()\n",
    "SASG_skip = SASG_data['Skip'].values.tolist()\n",
    "LASG_skip = LASG_data['Skip'].values.tolist()\n",
    "\n",
    "comm_round_TASGS = []\n",
    "comm_round_SASG = []\n",
    "comm_round_LASG = []\n",
    "comm_round_sparse = []\n",
    "comm_round_dis = []\n",
    "\n",
    "comm_bit_TASGS = []\n",
    "comm_bit_SASG = []\n",
    "comm_bit_LASG = []\n",
    "comm_bit_sparse = []\n",
    "comm_bit_dis = []\n",
    "\n",
    "Eopch_TASGS = []\n",
    "Eopch_SASG = []\n",
    "Eopch_LASG = []\n",
    "Eopch_sparse = []\n",
    "Eopch_dis = []\n",
    "\n",
    "\n",
    "\n",
    "comm_num_TASGS, comm_num_SASG, comm_num_LASG, comm_num_sparse, comm_num_dis = 0, 0, 0, 0 ,0\n",
    "Eopch_n_TASGS,Eopch_n_SASG,Eopch_n_LASG,Eopch_n_sparse,Eopch_n_dis=0,0,0,0,0\n",
    "for i in range(len(TASGS_skip)):\n",
    "    \n",
    "    comm_num_TASGS += original_num\n",
    "    a= comm_num_TASGS - TASGS_skip[i]\n",
    "#     print(\"总轮次\",a)\n",
    "    comm_round_TASGS.append(a)\n",
    "    \n",
    "    \n",
    "for i in range(len(SASG_skip)):\n",
    "    comm_num_SASG += original_num\n",
    "    b=comm_num_SASG - SASG_skip[i]\n",
    "\n",
    "    comm_round_SASG.append(b)\n",
    "\n",
    "for i in range(len(LASG_skip)):\n",
    "    comm_num_LASG += original_num\n",
    "    comm_num_sparse += original_num\n",
    "    comm_num_dis += original_num\n",
    "    \n",
    "    c=comm_num_LASG - LASG_skip[i]\n",
    "\n",
    "\n",
    "    comm_round_LASG.append(c)\n",
    "    comm_round_sparse.append(comm_num_sparse)\n",
    "    comm_round_dis.append(comm_num_dis)\n",
    "\n",
    "\n",
    "for k in range(20):\n",
    "    Eopch_n_TASGS += k\n",
    "    Eopch_n_SASG += k\n",
    "    Eopch_n_LASG += k\n",
    "    Eopch_n_sparse += k\n",
    "    Eopch_n_dis += k\n",
    "    \n",
    "    Eopch_TASGS.append(Eopch_n_TASGS)\n",
    "    Eopch_SASG.append(Eopch_n_SASG)\n",
    "    Eopch_LASG.append(Eopch_n_LASG)\n",
    "    Eopch_sparse.append(Eopch_n_sparse)\n",
    "    Eopch_dis.append(Eopch_n_dis)\n",
    "    \n",
    "font1 = {'weight': 'normal', 'size': 17}\n",
    "font2 = {'weight': 'normal', 'size': 20}\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_TASGS, sgd_data['test-acc'].values.tolist(), 'black', label='SGD', linewidth=2.5, linestyle='-')\n",
    "plt.plot(comm_round_SASG, sparse_data['test-acc'].values.tolist(), 'b', label='Sparse', linewidth=2.5, linestyle='-')\n",
    "plt.plot(comm_round_LASG, LASG_data['test-acc'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "plt.plot(comm_round_sparse, SASG_data['test-acc'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "plt.plot(comm_round_dis, TASGS_data['test-acc'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(40000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.ylim(75, 90)\n",
    "ymajorLocator = MultipleLocator(2)  # major\n",
    "ax.yaxis.set_major_locator(ymajorLocator)\n",
    "yminorLocator = MultipleLocator(1)  # minor\n",
    "ax.yaxis.set_minor_locator(yminorLocator)\n",
    "plt.ylabel('Test Accuracy', font2)\n",
    "\n",
    "\n",
    "\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.13, right=0.95, bottom=0.15, top=0.95)\n",
    "\n",
    "plt.savefig(\"./result/\"+\"test_full.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_dis, sgd_data['Loss'].values.tolist(), 'black', label='SGD', linewidth=2.5, linestyle='-')\n",
    "plt.plot(comm_round_sparse, sparse_data['Loss'].values.tolist(), 'b', label='Sparse', linewidth=2.5, linestyle='-.')\n",
    "plt.plot(comm_round_LASG, LASG_data['Loss'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "plt.plot(comm_round_SASG, SASG_data['Loss'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "plt.plot(comm_round_TASGS, TASGS_data['Loss'].values.tolist(), 'c', label='GSASG', linewidth=2.5)\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(40000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(20000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "# ax.ticklabel_format(axis='y', style='plain', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss', font2)\n",
    "# plt.ylim(0, 2)\n",
    "# ax.set_yticks((0.1, 1,)\n",
    "# ax.set_yticklabels([ '$10^{0}$', '$ 10^{-1}$'], fontsize=17)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.16, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"loss_full.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
